---
toc: FALSE
format:
  html: default
  pdf: default
  arxiv-pdf:
    keep-tex: true
engine: knitr
bibliography: bibliography.bib
title: From Text to Insight - A Novel Approach to Measuring Business Model Innovation
short-title: Hope
author:
  - name: Max Gabler
    email: max.gabler@uni-ulm.de
  - name: Wanshu Jiang
    email: wanshu.jiang@uni-ulm.de
  - name: Christoph Kiesl
    email: christoph.kiesl@uni-ulm.de
#    affiliation:
#      name: Institute for Clarity in Documentation
#      address: P.O. Box 1212
#      city: Dublin
#      state: Ohio
#      country: USA
#      postal-code: 43017-6221
  - name: Leonard Pöhls
    email: leonard.poehls@uni-ulm.de
keywords:
    - 10-K
    - Business Model Innovation
    - BERT
    - Gemini
  # teaser:
  #   image: sampleteaser
  #   caption: figure caption
  #   description: teaser description    
abstract: |
 The ability of a company to continuously innovate its business model is a pivotal determinant of long-term success in dynamic markets. It is therefore crucial to ensure the reliability of business model innovation  measurement. In this study, we analyze business descriptions from 10-K filings from a sample of 4492 different companies between 2017 and 2023, which we summarized using a large language model (LLM). The innovation of business models is measured on the basis of the annual changes in the business descriptions, with BERTScores serving as quantitative comparison indices for the business descriptions. Our findings indicate that above-market business model innovation positively influences a firm's sales growth but has a negative effect on Tobin's Q growth. Notably, small-cap companies experience significant sales growth linked to business model innovation. These insights highlight the potential of textual similarity in regulatory filings as a robust indicator of business model innovation, offering a novel approach to tracking such innovation over time.
---
{{< pagebreak >}}

```{r eval = FALSE, echo=FALSE, warning = FALSE, message = FALSE, results='asis'}
library(knitr)
library(glue)
library(gt)
if(!require(webshot2)) warning("you'll need this")
knit_print.gt_tbl = function(x, ...) {
  
  if(!knitr::is_latex_output()){
    return(gt:::knit_print.gt_tbl(x, ...))
  } 
  
  f <- glue("{opts_current$get('label')}-gt_tbl.png")
  x |> gtsave(filename=f, 
  zoom = 1,expand=5)
size <- dim(png::readPNG(f)) # overkill, probably better ways with magic
  knitr::asis_output(glue("\\centering\\includegraphics[width=<size[1]>\\pixel,height=<size[2]>\\pixel]{<f>}", .open="<",.close=">"))
}
registerS3method(
  "knit_print", "gt_tbl", knit_print.gt_tbl,
  envir = asNamespace("knitr")
)
```

# Introduction

Business model innovation (BMI) is a key activity to maintain competitiveness and even gain a competitive advantage in todays fast paced markets [@pucihar_drivers_2019; @teece_business_2018]. As a result, interest in BMI has grown rapidly over the past twenty years. In particular, research examining the impact of BMI on firm performance has been a prominent area of investigation, with numerous research papers published in this field [@cucculelli_business_2015; @latifi_business_2021; @zott_fit_2008; @white_exploring_2022]. While the financial literature offers a wide range of established methods for measuring a company's performance, the BMI literature provides only a limited number of measures, all of which face similar challenges [@white_exploring_2022]. Furthermore, these measures vary largely. In order to further validate and advance the BMI research field, more sophisticated and comprehensive measurement instruments are necessary [@huang_review_2023].

Scales and measures used in the BMI literature [@clauss_measuring_2017;@spieth_business_2016] provide managers and practitioners with a measurement index for BMI. But these measures only validate applicability of BMI theory [@huang_review_2023] and are insufficient for longitudinal studies [@clauss_measuring_2017]. Hence, these measures are not adequate for a time series analysis of BMI. We address this gap by proposing a novel approach to measuring BMI. US-based companies are obliged by the United States Security and Exchange Commission (SEC) to submit annual 10-K filings, wherein a detailed description of the company's business operations is required. Hoberg & Phillips [-@hoberg_text-based_2016], on which we build this study, use these filings to cluster companies into industries. We summarise these descriptions using the Large Language Model Gemini and calculate the BERTScore for each company in our sample over several years. This approach enables the measurement of changes in the business model (BM) over time as the distance between the BM summary of one year to another. In order to test the validity of our measure, we regress sales growth and Tobin's Q growth on our measure. Additionally, we create our own industry classification based on BERTScores of business descriptions of same firms within the same year.

We conduct multivariate regressions and find that our measure is positively correlated with firm performance, showing its validity. Furthermore, we find that BMI has a greater impact on firm performance for smaller companies than medium and large companies. Consequently, we contribute to the literature by introducing a novel approach of measuring BMI, which overcomes drawbacks from existing approaches. Our measure and approach enable researchers to study BMI in longitudinal and large-scale studies. Lastly, we introduce our own industry classification based on a company's BM, which represents an alternative to existing ones.

The SEC mandates that the majority of public companies based in the United States submit an annual 10-K filing. In the first section a company presents its general business, encompassing information about its products and services. In some instances additional topics may be addressed, such as labor issues or competition [@sec_investor_2024]. In conclusion, this section contains the most useful information for describing a company's BM [@lee_business_2014]. Furthermore, 10-K filings are a reliable source of information, given that US law prohibits false or misleading statements in the filings. The SEC monitors the compliance of the companies with the requirements and comments where disclosure appears to be inconsistent [@sec_investor_2024]. Because of these guidelines, these descriptions are particularly suitable for a text analysis in order to quantify a BM. 

Despite the growing interest in BMI and the increasing number of theoretical and empirical studies in this field, the research of BMI is still in a preliminary state [@huang_review_2023]. Consequently, there is considerable variation in the definitions of BMI, with some definitions being more similar to one another than others [@foss_fifteen_2017]. Spieth & Schneider [-@spieth_business_2016] identify three core dimensions that comprise a company's BM: its value proposition, its value creation architecture and its revenue model logic. Based on this, BMI can be conceptualized as a change that is new-to-the-firm in at least one of these dimensions. Furthermore, Spieth and Schneider [-@spieth_business_2016] introduce a measurement model to evaluate these three dimensions of BMI. They develop an index by first specifying the contents, followed by a specification of the indicators and assessing their content validity, assessing the indicators collinearity and finally assessing the external validity. Clauss [-@clauss_measuring_2017] employs a very similar approach. After specifying the domain and dimensionality of BMI through literature research, the author divides his scale into three hierarchical levels, which are very similar to the ones of Spieth and Schneider [-@spieth_business_2016]. We build on these conceptualizations to design our prompt we use in the pre-processing with Gemini. However, both measures are subject to three significant limitations. Firstly, both measures lack a temporal component. Consequently, they are inadequate for use in longitudinal studies or ex-post evaluations of BMI. Secondly, BMI is only measured at the new-to-the-firm level rather than at the new-to-the-industry or new-to-the-market level. Thirdly, both measures rely on interviews and questionnaires, which makes conducting large-scale studies time-consuming and reliant on the willingness of the companies to cooperate [@clauss_measuring_2017; @spieth_business_2016]. Our novel measurement approach tackles the first and third issue. 

A number of studies have examined the relationship between BMI and the financial performance of a company. Cucculelli & Bettinelli [-@cucculelli_business_2015] investigate the effect of BMI on sales growth, return on sales (ROS) and total factor productivity (TFP). The results support the hypothesis that BMI has a positive effect on firm performance, with the effect increasing in line with the intensity of the innovation. Desyllas et al. [-@Desyllas2022] find that BMI has a small effect on performance of incumbent firms. They measure firm performance by Tobin's Q growth. White et al. [-@white_exploring_2022] conducted a meta-analysis based on the extant BMI literature. They found a positive relationship between BMI and firm performance, and that this relationship is shaped by factors including the firm age, industry, the economic and political environment and BMI characteristics. Based on this, we derive the dependent and control variables in the estimation strategy.

Hoberg & Phillips [-@hoberg_text-based_2016] present a novel approach to defining industry boundaries. They propose two novel industry classification methods: the fixed industry classification (FIC) and the text-based network industry classification (TNIC). Firstly, they cluster companies based on the similarity of word vectors into fixed industries. Secondly, they define a minimum similarity threshold, above which firms are considered in the same industry. This second step relaxes their prior properties of binary membership transitivity and fixed industry location. The authors demonstrate shortcomings in the traditional industry classification systems such as the Standard Industry Classification (SIC) and the North American Industry Classification System (NAICS), which are not able to account for temporal changes. The new method is capable of capturing changes in industry boundaries and competitor sets over time, thereby providing a dynamic industry classification system. Based on the FIC we propose our own BERTScores industry classification and utilize it in our estimation. In their study, Lee & Hong [-@lee_business_2014] examine the evolution of a firm's BM over time. After filtering the Item 1 parts of the 10-K filings for relevant sentences, Lee & Hong [-@lee_business_2014] construct keyword vectors, which represent the concept of the BM. Therefore, the evolution of the BM is depicted as the change in the distribution of keywords over time. The authors advocate for a more robust methodology, such as incorporating multi-word phrases in the keyword vectors, to enhance the reliability of the approach [@lee_business_2014]. Our study pursues a similar goal but with a novel methodology.

The remainder of the paper is organized as follows. Section 2 describes the origin and preparation of our data, the use and functioning of BERT, the preprocessing with Gemini and our methodology. Section 3 lays out our estimation strategy. Section 4 contains our results and discussion. Section 5 concludes our study.

{{< pagebreak >}}

# Data and Methodology

## The Dataset

We collect 10-K filings from the digital SEC Database, using the category "10-K" as extraction condition. Since the focus of our study lies on company's BM, we merely use the Item 1 part, since this is the most crucial part of the 10-K filings for describing the companies BM [@lee_business_2014]. Our observations are limited to an intersection of such companies, which has been made available to the SEC since 2001 in a publicly accessible database. We extracted 10-K filings that were submitted between 2017 and 2023 based on underlying Central Index Keys (CIK). Occasionally, such filings are submitted retrospectively or are already submitted for the same year. We are therefore limiting the period for which we are reporting to 2016-2023, with fewer observations available for 2016 and 2023 as a result. We exclude companies from the financial sector, namely companies with a SIC Code starting with six. Corresponding to Table 2,

```{r echo=FALSE, warning = FALSE, message = FALSE, results='asis'}
library(tidyverse)
library(knitr)
library(kableExtra)
library(readxl)

Table1 = read_excel("./TableData/Table1.xlsx")%>%
  mutate(V3 = as.character(V3))%>%
  replace_na(list(V1="",V2="",V3=""))%>%
  kable(format = 'latex',label="tbl-table1",col.names = c("Source/Filter","Sample Size","Observations Removed"), caption = "10-K Sample Creation", booktabs = T)%>%
  kable_styling(latex_options = c("scale_down","HOLD_position"),full_width = T) %>%
  footnote("Filings submitted between 2017 and 2023 are considered.")%>%
  column_spec(1, width = "8cm")

Table1
```

multiple steps of pre-processing were required to obtain the final amount of 21,417 observations for seven years. Financial key figures, including sales, total assets, market values and Tobin's Q were extracted from DataStream. A total of 4,494 companies are included in the sample, although the availability of filings could not always be guaranteed for all years. This is due on the one hand to the quality of the API to the SEC and on the other hand to companies that did not file 10-K reports or were listed on the stock exchange for the entire period under review. Furthermore, other financial databases (e.g. Compustat) are better suited for mapping with the SEC, as Compustat in particular has a better mapping match for identifiers [^1]. Ultimately, we have access to the financial key figures of the companies for the respective year, the Item I text pre-processed with the help of Gemini, company-specific identification features and the conventional SIC industry classification. The information on SIC sector classification is limited to companies that are currently actively filing. Therefore, 609 firms are no longer actively filing, e.g. due to company bankruptcies or mergers.

[^1]: The University of Wharton provides an official mapping from gvkey to cik: https://wrds-www.wharton.upenn.edu/login/?next=/pages/get-data/wrds-sec-analytics-suite/wrds-sec-linking-tables/gvkey-cik-link-table/

## BERT and BERTScore

BERT is a pre-trained and transformer-based model for natural language processing (NLP) based on artificial neural networks. It works according to the transformer architecture, first mentioned by Vaswani et al. [-@vaswani2017attention]. Unlike Hoberg & Philips' [-@hoberg_text-based_2016] word-to-vec approach, BERT operates bidirectionally, considering the context from both sides of each word simultaneously. This bidirectional design helps clarify word meanings based on context, resulting in more accurate similarity calculations between texts that may use the same words but convey different meanings. BERT can also be fine-tuned for specific tasks, making it adaptable to different datasets, improving its performance even with limited labeled data. Thus, BERT effectively captures deeper semantics in texts such as 10-K reports. The BERTScore, a metric built on BERT embeddings, computes cosine similarity between word or text meanings learned by BERT, where a scale from -1 to 1 is used, with 1 representing perfect similarity.

In our study, we employ the 'bert-base-uncased' model in two distinct but complementary tasks. First, for inter-company comparisons, we use BERT to generate embeddings for business descriptions from different companies by processing each entry. The BERT tokenizer converts input text into tensors, and the model generates embeddings by averaging the token embeddings from the last hidden state. We finally compute pairwise cosine similarity scores between them to assess semantic similarity across companies' business descriptions. In parallel, we apply a similar approach to assess year-to-year evolution of business descriptions within individual companies. For each company, we retrieve business descriptions from two consecutive years and generate embeddings using the same BERT model. Here, we utilize the BERTScore metric to compare descriptions from adjacent years, such as 2017 and 2018. The similarity is calculated based on precision (P), recall (R), and F1-score, with the F1-score serving as the primary measure. This year-to-year comparison enables us to analyze how business descriptions evolve over time.

## Preprocessing with Gemini

10-K filings are typically very large text documents, and Item 1 of these filings is no exception. Table 2 shows the descriptive measures of the length of the original Item 1 section in our final sample. The length of a document was measured by the word count without punctuation. The document length ranges from a minimum if 49 words up to 78,799 words. On average the documents are between 6,626 and 10,304 words long. In order to utilize the entirety of the information regarding the BM in the 

```{r echo=FALSE, warning = FALSE, message = FALSE, results='asis'}
library(tidyverse)
library(knitr)
library(kableExtra)
library(readxl)

Table2 = read_excel("./TableData/Table2.xlsx")%>%
    kable(format="latex",label="tbl-table2",col.names = c("Report-for Year","Average Word Count","Standard Deviation","Minimum","25th Percentile","Median","75th Percentile","Maximum"),booktabs = T, caption = "Descriptive Statistics of Number of Words in Original Filings")%>%
  kable_styling(latex_options = c("scale_down","HOLD_position"))%>%
  footnote("All 21,417 original filings were considered.")

Table2
```

Item 1 section and pass the text to our BERT model, we decided to let Google's GenAI chatbot Gemini summarize them to a maximum length of 512 tokens. The summaries were created between 26 June 2024 and 6 August 2024. The model employed was Gemini Flash 1.5. The prompt was inserted at the beginning of each text file and it was passed via an API to Gemini [^2]. Our prompt covers all aspects of the definition of BMI proposed by Spieth & Schneider [-@spieth_business_2016]. For more details, see Appendix A. To assess the quality and accuracy of the summaries produced by Gemini, a random sample of 100 filings was selected for comparison with the original text. More precise,the original file was initially read with a focus on the points mentioned in the prompt. Subsequently, the summary was evaluated to ascertain whether it contained these same points. A list of the sample with the summaries is provided in the Appendix B.

[^2]: We forked and used following Github repository: https://github.com/skranz/gemini_ex.

## Methodology

From the dataset containing all the summaries and financials per company per year we construct three datasets for our further analysis. The first dataset is used for the BERTScore industry classification. We therefore filter all summaries by the first year in which the company submitted a filing (usually 2017). We then calculate the BERTScores between all these summaries. The second and third dataset are used for the regression analysis of our measure. In the second dataset, we fix the company and calculate the BERTScore of the summaries of one year and its direct consecutive year as well as the sales growth rate and the Tobin's Q growth rate (according to equation 2 and 3) . We merge this dataset with our BERTS scores calculated in pairs between the same companies in different years. 

$$
\hspace*{-6cm}
\begin{array}{@{}l}
\text{(1) Sales Growth} = \left(\frac{\text{Revenue}_t}{\text{Revenue}_{t-1}} - 1\right) \times 100
\\[0.5cm]
\text{(2) Tobin's Q Growth} = \left(\frac{\text{Tobin's Q in year }_t}{\text{Tobin's Q in year }_{t-1}} - 1\right) \times 100
\end{array}
$$

$\text{for } t \in \{2017;2023\}$. The first dataset is utilized to compute our BERTScore industry classification. We firstly compare the BERTScore industry classification with the FIC by Hoberg & Phillips [-@hoberg_text-based_2016] and the SIC. Furthermore, we use our BERTScore industry classification in our estimation. For the FIC, Hoberg & Phillips [-@hoberg_text-based_2016] calculate the cosine similarity between word vectors of product descriptions, which they extracted from Item 1 of the 10-K filings. For our Industry classification we utilize the BERTScore to calculate the similarity between our BM summaries. Based on these similarities we cluster the companies into industries via an agglomerative clustering algorithm. The methodology and object of research differ between the two studies. In accordance with the definition provided by Spieth & Schneider [-@spieth_business_2016], a company's product constitutes a component of its value proposition and, consequently, a constituent of the BM. Because the product is thereby entangled with the BM, companies that have similar products might have similar BMs. So despite the different methodology and object of research, we expect a similar distribution as Hoberg for the FIC, which is very granular and contains lot of small industries. Thus, we hypothesize:

  **H1**: Our BERTScore industry classification shows a similar distribution compared to the FIC. 
  
  **H2**: Our BERTScore industry classification has a high overlap with the FIC.

As mentioned, our approach differs from the original paper by Hoberg & Phillips [-@hoberg_text-based_2016]. We fix the company and calculate the BERTScore of the summaries of one year and the following year. When a company innovates its BM over time, the 10-K filings change and thus the summaries of these filings. We subtract the BERTScores from one to get the distance instead of the similarity between summaries, because the distance yields a more intuitive interpretation: The higher the distance, the more do the BM summaries differ. Figure 1 shows the density function of the distance. The distance looks normally distributed and has a mean of 0.104 and a standard deviation of 0.027.  

```{r echo=FALSE, warning = FALSE, message = FALSE, results='asis'}
#| label: fig-1
#| echo: false
#| fig-cap: "Density Function of the Distance"
library(tidyverse)
library(ggplot2)

Figure1 <- read.csv("./TableData/Regression/RegressionData.csv")%>%
  select(c("Score")) 

Mean = mean(1-Figure1$Score)
SD = sd(1-Figure1$Score)
min = min(1-Figure1$Score)
max = max(1-Figure1$Score)

Figure1 = Figure1%>%
  ggplot(aes(x=1-Score)) +
  geom_density(alpha = 0.5, fill="grey") +
  labs(caption= "Note:\nOnly distances for same company between different years.",x = "Distance", y = "Density") +
  theme_minimal()+
  geom_vline(xintercept=Mean,alpha=0.8)+
  annotate("text", x=Mean+0.002, y=14.9, label="Mean", angle=90, size= 3)

Figure1
```

The values range from zero to 0.216. This means that on average the summaries of a company differ slightly. We attribute these small differences to our preprocessing rather than that companies on average slightly change their BM every year. Even if the summaries are very similar in terms of content, Gemini might use different phrases and wordings which result in different BERTScores and thereby in a higher distance. Hence, we normalize the distance by dividing by its mean. Furthermore, we subtract one and multiply it by one-hundred in order to further ease the interpretation of the coefficient. This results in the following definition of our measure: 

$$
\hspace*{-6cm}
\begin{array}{@{}l}
\text{(3) Normalized Distance} = \left( \frac{\text{Distance}_i}{\frac{1}{n} \sum_{i=1}^{n} \text{Distance}_i} - 1 \right) \cdot 100
\end{array}
$$

where n denotes the number of observations in the dataset. The Normalized Distance measures the difference in the distance between two consecutive summaries of some company above the market average in percentage points. Under the assumption that the summaries reliably represent the firms' BM, the Normalized Distance measures the change of a company's BM above the market average in percentage points from one year to the next. We further assume that a company that changes its BM, will not return back to its original BM. Under this assumption the Normalized Distance measures BMI on the new-to-the-firm level. The literature posits that changes in the business model can be positively correlated with firm performance. In the case that our measure is able to measure BMI, we expect to find a positive relationship between our BMI measure and firm performance. Therefore, we hypothesize that:

  **H3**: Our measure for BMI is positively correlated with firm performance.
  
When measuring BMI, Guldmann & Huulgaard [-@guldmann_barriers_2020] assume several barriers to BMI in the circular economy, comparing differences between company sizes and equal company sizes. They found that even same-sized companies face different barriers to BMI, but also identify company size, industry and customer segments as drivers of differences in barriers to BMI. There is a gap in the literature when it comes to investigating the effect of BMI on the performance of different company sizes. We expect higher effects for companies with lower market capitalization (according to Guldmann & Huulgaard [-@guldmann_barriers_2020], smaller companies are more flexible in changing established processes), while companies with medium or high market capitalization tend to show a less strong effect, which is why we put forward the following hypothesis:

  **H4**: BMI has a stronger positive effect on the performance of small companies compared to medium and large companies.
  
# Estimation Strategy

We test H3 using multivariate regression techniques. The independent variable is the Normalized Distance, as previously defined. Two different dependent variables are employed in order to measure a company's performance. In the initial specification, the sales growth is used as the dependent variable by Cucculelli & Bettinelli [-@cucculelli_business_2015]. For this specification, the second dataset is employed, and all observations with zero values for market value and total assets were removed before applying logarithms. We control for firm size by using the logarithm of the market value once and the logarithm of total assets once [@dang2018measuring]. Moreover, outlier values are winsorized above the 99th percentile. For a more detailed discussion of this step, see Appendix C. The following regressions are used to estimate the normalized distance:

$$
\begin{array}{l}
\text{(4) } Y_{\text{Sales Growth}} = \beta_0 + \beta_1 X_{\text{Normalized Distance}} + \beta_2 \ln(X_{\text{Market Value}}) + \gamma_{\text{year} \times \text{industry}} + \epsilon_{it}
\\[0.5cm]
\text{(5) } Y_{\text{Sales Growth}} = \beta_0 + \beta_1 X_{\text{Normalized Distance}} + \beta_2 \ln(X_{\text{Total Assets}}) + \gamma_{\text{year} \times \text{industry}} + \epsilon_{it}
\\[0.5cm]
\text{(6) } Y_{\text{Tobin's Q Growth}} = \beta_0 + \beta_1 X_{\text{Normalized Distance}} + \beta_2 \ln(X_{\text{Market Value}}) + \gamma_{\text{year} \times \text{industry}} + \epsilon_{it}
\\[0.5cm]
\text{(7) } Y_{\text{Tobin's Q Growth}} = \beta_0 + \beta_1 X_{\text{Normalized Distance}} + \beta_2 \ln(X_{\text{Total Assets}}) + \gamma_{\text{year} \times \text{industry}} + \epsilon_{it}
\end{array}
$$

We use the interaction term $\gamma$ of the year and our BERTScore industry classification as fixed effects. The interaction term of the year and the industry is employed to capture potential heterogeneous effects of the industry classification across different years. The economic and political environment is not controlled for, as suggested by White et al. [-@white_exploring_2022], since all the companies in the study are based in the United States.
In the second specification, the growth of Tobin's Q is used as described in equation (2). For the calculation of the growth rate of Tobin's Q, the same data set was used as for the calculation of the sales growth rate. In addition, we use fixed year and industry effects to control for heterogeneous effects between industries interacting with the respective year. As in the first specification, we do not control for the economic and political environment. We use a similar multivariate regression model to test H4:
$$
\begin{array}{l}
\text{(8) }Y_{\text{Sales Growth, k}} = \beta_{0, k} + \beta_{1, k} X_{\text{Normalized Distance}} + \beta_{2, k} \ln(X_{\text{Total Assets}}) + \gamma_{\text{year} \times \text{industry}} + \epsilon_{it, k}
\\[0.5cm]
\text{(9) }Y_{\text{Tobin's Q Growth, k}} = \beta_{0, k} + \beta_{1, k} X_{\text{Normalized Distance}} + \beta_{2, k} \ln(X_{\text{Total Assets}}) + \gamma_{\text{year} \times \text{industry}} + \epsilon_{it, k}
\\[0.5cm]
\end{array}
$$

$\text{whereby } k \in \{\text{Small-Cap}, \text{Mid-Cap}, \text{Large-Cap}\}$. We divide the dataset into three different groups, using a flat split of market capitalization defined by NASDAQ as the thresholds. Accordingly, companies with a market capitalization of up to one billion US dollars are assigned to small-cap firms, companies up to five billion US dollars to mid-cap firms and companies larger than that to large-cap firms. As in the first specification, we use the interaction term of the year and our BERTScore industry classification as a fixed effect and control for firm size via the logarithm of the total assets.

# Results and Discussion
## Comparison

Our study builds on the idea of Hoberg & Phillips [-@hoberg_text-based_2016] to utilize text data from 10-K filings to classify companies based on their product similarity into dynamic industries. They achieve this through the parsing of the product descriptions provided by Item 1 of firms 10-K filings and creating word vectors. Specifically, the authors identify and exclude proper nouns, which include common words and geographic locations. They then create word vectors for each firm and year, which enables the measurement of product similarity over time. They perform two steps to create the FIC. Firstly, a hierarchical agglomerative clustering algorithm is employed to cluster companies based on their similarity and maximize ex-post within cluster similarity. This enables a classification with any number of clusters. In the second step, the authors compute aggregated word vectors for each industry. These vectors now represent the industries. Subsequently, the similarity between industries and firms is calculated for each of the following years. From the second year onwards, firms are classified according to the industry with which they are most similar. Our approach differs in two ways: Firstly, in contrast to the TNIC and FIC, which employ word-to-vec, our approach utilizes BERT to represent text, which allows us to capture the context of words. Accordingly, the BERTScore is employed instead of the cosine similarity as our similarity measure. Secondly, our analysis is focused on the description of the BM rather than on the product descriptions. Nevertheless, in the following subsection, the BERTScore industry classification is compared with the FIC and the SIC.
We employ the first dataset for the BERTScore industry classification and the comparison. The SIC codes come from the SEC website[^3]. For the FIC we have utilized the similarity scores provided by Hoberg-Phillips Data Library[^4]. The data consists of the gvkeys of two companies, the year and the cosine similarity between these two companies. In order to ensure comparability, only companies present in both the present study's dataset and that provided by the authors are included in the analysis. Because we use CIKs and accession numbers to identify firms and filings, and the fact that the data library employs Compustat's gvkeys, the matching of CIKs with gvkeys inevitably results in the loss of some observations. Ultimately, for the comparison the clustering algorithm was applied to 1,958 of the 3,246 firms in our sample for the year 2017. In our dataset, companies are from 320 different SIC codes. Therefore, for the comparison the number of industries chosen for our industry classification and the FIC is 320.

Figure 2 compares the distribution of industry size for the BERTScore classification, the FIC and the SIC. Both the BERTScore classification and the FIC show a similar distribution, displaying a leftward skew with the majority of industries comprising fewer than ten firms. 

```{r echo=FALSE, warning = FALSE, message = FALSE, results='asis'}
#| label: fig-2
#| echo: false
#| fig-cap: "Distribution Comparison between BERT Classification, FIC and SIC"
library(tidyverse)
library(ggplot2)

#Load and edit data
Figure2 = read.csv("./TableData/Figure2.csv") %>%
  mutate(Firms = ifelse(Firms>50,50,Firms))%>%
  mutate(Clustering = ifelse(Clustering == "Hoberg","FIC",Clustering))

#create plot
Figure2 = Figure2 %>%
  ggplot(aes(x = Firms, fill = Clustering)) +
  geom_histogram(alpha = 1, bins = 50, col="grey") +
  scale_x_continuous(breaks = seq(0, 50, by = 5), labels = c(seq(0, 45, by = 5), "50+")) +
  facet_wrap(~ Clustering, ncol = 1) +
  labs(
    x = "Number of Firms in Industry",
    y = "Frequency",
    fill = "Gruppe"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5),
    axis.title.x = element_text(size = 12),
    axis.title.y = element_text(size = 12),
    legend.position = "none"
  )

Figure2
```

The SIC shows as well a left skewed distribution but with most industries only containing one company. The distribution of the FIC is steeper than the one of the BERTScore classification. It is notable that the largest industry in the BERTScore classification comprises only 20 companies, whereas the FIC and SIC contain industries with a greater number of firms, with some exceeding 50. This suggests that the BERTScore classification groups small to medium-sized industries, comprising between two and fourteen firms per industry, with fewer large industries. The FIC also comprises mostly of small to medium-sized industries, with a few larger ones. Despite these minor differences, this supports H1. The degree of homogeneity between the BERTScore classification and the FIC is 0.63, while the completeness is 0.6. This demonstrates only a medium degree of overlap between the two classifications. The Adjusted Rand Index (ARI) [@hubert_comparing_1985] is situated at 0.0002, which is close to zero, indicating that the overlap might be random. These findings do not provide support for H2.

In order to use our BERTScore industry classification in our estimation, we classify all 3,246 companies from the year 2017 as described above. Since we use BERT and do not have word vectors for each industry, our methodology differs in the second step from Hoberg & Phillips [-@hoberg_text-based_2016]. We assign the remaining companies of which we do not have data for the year 2017 to the industry of the company which are already classified and which they are most similar to.

[^3]: The list can be found here: https://www.sec.gov/search-filings/standard-industrial-classification-sic-code-list. 
[^4]: For the database see: https://hobergphillips.tuck.dartmouth.edu.

{{< pagebreak >}}

## Results

Table 3 presents the four regressions related to hypothesis H3. As previously described, this relationship is examined using sales growth and Tobin's Q growth as firm performance indicators. Models 1 and 2 use sales growth as the dependent variable. Model 1 shows a significant positive effect for the Normalized Distance ($\hat{\beta_1}$ = 0.332, p < 0.001) on the sales growth. The results can be interpreted as that a company, that changes its BM in one year one percentage point more than the market average, experiences on average 0.289 percentage points more growth in sales. Model 2 as well shows this significant positive effect ($\hat{\beta_1}$ = 0.326, p < 0.001). Both regressions yield very similar results regardless of whether we include the logarithm of the market value ($\hat{\beta_2}$ = -0.476, p > 0.1) or the logarithm of the total assets ($\hat{\beta_1}$ = -1.790, p < 0.05) as a control variable for the firm size. These results provide support for H3. Model 3 and 4 assess the effect of the Normalized Distance on Tobin's Q growth. Both Models show a negative significant effect ($\hat{\beta_1}$ = -0.098, and $\hat{\beta_1}$ = -0.233). This can be interpreted as indicating that a company that changes its BM by one percentage point more than the market average over the observation period will experience, on average, a reduction in Tobin's Q growth of 0.233, respectively 0.098, percentage points. As in Model 1 and 2, the coefficients for the Normalized Distance only change very little when controlling for the logarithm of the market value instead of the logarithm of the total assets. The results do not support H3, since we anticipated a positive effect.

```{r echo=FALSE, warning = FALSE, message = FALSE, results='asis'}
#Main Regression Results
library(tidyverse)
library(readxl)
library(lfe)
library(DescTools)
library(modelsummary)
library(kableExtra)
library(tibble)

RegressionData = read.csv("./TableData/Regression/RegressionData1.csv") %>%
  filter(MV>0, Total.Assets >0)%>%
  mutate(year=as.factor(year),
         Industry = as.factor(Industry))

RegressionData$RevenueGrowth = Winsorize(RegressionData$RevenueGrowth,probs=c(0,0.99))
RegressionData$TobinsQGrowth = Winsorize(RegressionData$TobinsQGrowth,probs=c(0,0.99))


#Revenue Growth
Reg1 = felm(RevenueGrowth~NormalizedDistance + log(MV) | year * Industry | 0 | 0 ,data=RegressionData)
Reg2 = felm(RevenueGrowth~NormalizedDistance + log(Total.Assets) | year * Industry | 0 | 0 ,data=RegressionData)

#Tobin's Q
#TobinsQ = read.csv("./TableData/Regression/TobinsQGrowth.csv") %>%
#    filter(MeanMV>0, MeanAssets >0,TimeDiff>0)%>%
#    mutate(Industry = as.factor(Industry),
#           TimeDiff = as.factor(TimeDiff))

Reg3 = felm(TobinsQGrowth~NormalizedDistance + log(MV)| year * Industry | 0 | 0 ,data=RegressionData)
Reg4 = felm(TobinsQGrowth~NormalizedDistance + log(Total.Assets) | year * Industry | 0 | 0 ,data=RegressionData)

#model summary output
rows <- tribble(~term, ~Reg1, ~Reg2, ~Reg3, ~Reg4,
                'Industry x Year Fixed Effects', 'Yes', 'Yes', 'Yes', 'Yes')
#                'Industry Fixed Effects', 'No', 'No', 'Yes', 'Yes',
#                'Time Difference Fixed Effects', 'No', 'No', 'Yes', 'Yes')
attr(rows, 'position') <- c(8)

modelsummary(models = list("(1)" = Reg1, "(2)" = Reg2, "(3)" = Reg3, "(4)" = Reg4),
             output = "kableExtra",
             title = "Regression Results I",
             coef_map = c("NormalizedDistance" = "Normalized Distance", "log(MV)" = "log(Market Value)", "log(Total.Assets)" = "log(Assets)", "log(MeanMV)" = "log(Mean Market Value)", "log(MeanAssets)" = "log(Mean Assets)"),
             gof_map = c("nobs", "r.squared"),
             stars = TRUE,
             add_rows = rows
             )%>%
  add_header_above(c(" " = 1, "Sales Growth" = 2, "Tobin's Q Growth" = 2)) %>%
  kable_styling(latex_options = c("scale_down","HOLD_position"))%>%
  footnote("Industry classification by BERTScore.")

```

The findings indicate that BMI has a significant impact on firm performance. Specifically, BM changes have a positive effect on operational metrics such as sales growth, which can be viewed as a positive outcome. A differentiated view of market capitalization shows that the BMI of smaller companies predominantly has a positive impact on sales growth. In contrast, medium-sized companies have a negative effect, while large companies only show a marginal positive effect. However, the stock market does not respond as favorably to such changes. Tobin's Q growth decreases when companies adjust their BM, meaning the ratio of market value to total capital declines. This effect is not necessarily problematic, as market value is influenced by stock trading, which incorporates subjective evaluations that are reflected in this metric. Table 4 illustrates how the previously demonstrated effects influences individual firm sizes. When we differentiate the companies into Small-cap (6,459 observations), Mid-cap (2,904 observations), and Large-cap (2,651 observations), the explanatory coefficients reveal substantial differences. The small firms exhibit by far the most significant effect on the altered distance, which is also significant at the 1% level. In contrast, for large companies, we observe only approximately 20% of the effect of the BM change on sales figures. Furthermore, the coefficient does not even exceed one standard deviation, indicating a lack of significance. The situation is even less favorable for mid-sized companies, as evidenced by both the coefficient and standard deviation. The value of the distance effect accounts for merely 0,008% of that observed in large companies. Therefore, we can conclude that the previously observed effect across all firms can primarily be attributed to small firms. For medium to large firms, no discernible impact is evident. In the overall analysis, the effects are significant and support H4.

```{r echo=FALSE, warning = FALSE, message = FALSE, results='asis'}

RegressionData = read.csv("./TableData/Regression/RegressionData1.csv") %>%
  filter(MV>0, Total.Assets >0)%>%
  mutate(year=as.factor(year),
         Industry = as.factor(Industry))

RegressionData$RevenueGrowth = Winsorize(RegressionData$RevenueGrowth,probs=c(0,0.99))
RegressionData$TobinsQGrowth = Winsorize(RegressionData$TobinsQGrowth,probs=c(0,0.99))

RegressionData <- RegressionData %>%
 mutate(FirmSize = case_when(
  MV < 1000 ~ "Small_Cap", 
   MV >= 1000 & MV < 5000 ~ "Mid_Cap", 
   TRUE ~ "Large_Cap"
 ))

RegressionData <- RegressionData %>%
 group_by(cik) %>% 
 mutate(SameFirmSize = ifelse(n_distinct(FirmSize) == 1, 1, 0)) %>%
 ungroup()

Reg1 <- felm(RevenueGrowth~NormalizedDistance + log(Total.Assets) | year * Industry | 0 | 0 ,data=filter(RegressionData,FirmSize == "Small_Cap"))
Reg2 <- felm(RevenueGrowth~NormalizedDistance + log(Total.Assets) | year * Industry | 0 | 0 ,data=filter(RegressionData,FirmSize == "Mid_Cap") )
Reg3 <- felm(RevenueGrowth~NormalizedDistance + log(Total.Assets) | year * Industry | 0 | 0 ,data=filter(RegressionData,FirmSize == "Large_Cap"))

Reg4 <- felm(TobinsQGrowth~NormalizedDistance + log(Total.Assets) | year * Industry | 0 | 0 ,data=filter(RegressionData,FirmSize == "Small_Cap"))
Reg5 <- felm(TobinsQGrowth~NormalizedDistance + log(Total.Assets) | year * Industry | 0 | 0 ,data=filter(RegressionData,FirmSize == "Mid_Cap") )
Reg6 <- felm(TobinsQGrowth~NormalizedDistance + log(Total.Assets) | year * Industry | 0 | 0 ,data=filter(RegressionData,FirmSize == "Large_Cap"))

rows <- tribble(~term, ~Reg1, ~Reg2, ~Reg3, ~Reg4, ~Reg5, ~Reg6,
                'Industry x Year Fixed Effects', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes')
attr(rows, 'position') <- c(5, 6, 7, 8, 9, 10)

modelsummary(models = list("Small-Cap" = Reg1, "Mid-Cap" = Reg2, "Large-Cap" = Reg3, "Small-Cap" = Reg4, "Mid-Cap" = Reg5, "Large-Cap" = Reg6),
             output = "kableExtra",
             title = "Regression Results II",
             coef_map = c("NormalizedDistance" = "Normalized Distance", "log(Total.Assets)" = "log(Assets)"),
             gof_map = c("nobs", "r.squared"),
             stars = TRUE,
             add_rows = rows
             )%>%
  add_header_above(c(" " = 1, "Sales Growth" = 3, "Tobin's Q Growth" = 3)) %>%
  kable_styling(latex_options = c("scale_down","HOLD_position"))%>%
  footnote("Industry classification by BERTScore.")

```

The findings of this study contribute to the expansion of the considerations put forth by Hoberg & Phillips [-@hoberg_text-based_2016] Using BERT enables a semantic analysis of 10-K filings, allowing for a comprehensive company comparison. The categorization into industries can be updated annually, similar to Hoberg & Phillips [-@hoberg_text-based_2016] approach, albeit with a somewhat greater effort involved. However, this additional effort is justifiable, as the preliminary work with Gemini facilitates a holistic examination of the 10-Ks. This aspect is a significant advantage of our approach, which we believe surpasses the Word-to-Vec method employed by Hoberg. We fully utilize the description of the BM, although certain limitations may exist. However, the loss associated with the previous method is substantially greater, as most words do not contribute to the analysis at all.

To broaden the scope of this study, we also address the financial aspects of the companies and how innovations impact these aspects. The previously applied model will be used to measure changes in the BM. This allows for the testing of new hypotheses and the generation of additional insights. In this study, the relationship with performance is measured using revenue growth and Tobin's Q. We find a significant dependence of innovation on sales and Tobin's Q, indicating that while revenue increases, the market value to asset ratio declines with increased changes in the BM. The result of revenue growth is not surprising; we anticipated this effect based on the premise that revenues would rise as companies aim to capture new markets and thereby generate more sales through changes in their business plans. Conversely, the decline in Tobin's Q seems unusual. A possible explanation for this could be that investors trading stocks are often risk-averse, perceiving the changes negatively rather than viewing them as opportunities (time-varying risk aversion). Consequently, demand may decrease, and the stock price may also experience negative shifts. Furthermore, the profits generated by the company may temporarily decline due to these decisions. While the company’s revenue is projected to increase on average, innovations may incur additional expenses that diminish profit and, consequently, dividends. This could further reduce demand for the stock. Additionally, the new BM may create tensions with the existing one, leading to increased workload and other implications. In our further analysis, we differentiate the companies by size to examine whether the effects of revenue growth vary accordingly. This assumption is confirmed, as significance is measured exclusively among smaller firms with a market capitalization of less than one billion. This implies that changes in the BM do not affect the performance of revenue growth in medium to large firms. A potential reason for this could be that smaller companies typically possess a less extensive product portfolio, making changes to a business segment or the addition of a new product more impactful.

It should be noted that our approach is not without certain limitations. While using the BERTScore compels us to shorten and condense the descriptions of the BM, it integrates semantics into the similarity calculation. As a result, the entire text is considered, which appears to provide better informational value compared to solely relying on frequently occurring nouns. However, one potential critique of BERT is its limited ability to recognize implicit and subtle meanings [@acheampong_transformer_2021]. The approach can only process the textual descriptions. In this work, we exclusively examine BMs that provide little insight into current success in practice. Notably, there are no clear objective statements, as with many key financial indicators, that can be categorized as positive or negative [@george_business_2011]. A positive description of negative aspects is unlikely in this context, which justifies the use of the BERTScore. The use of BERT, and the consequent limitation regarding text length, posed additional challenges in this research. In recent years, the possibilities for utilizing AI have grown immensely. With the help of GitHub Actions, we were able to obtain suitable access to Gemini. This brings us to the most critical point of this work. This approach raises transparency issues for the results. The system does not provide explainable intelligence, meaning we cannot fully verify how exactly the texts are generated. Our only option is to delegate the task and trust the results’ accuracy. Nevertheless, we consider the current approach as the best possible solution for condensing our BMs. On the one hand, the extracted texts do not follow the same structure across companies, and on the other, cutting the BMs arbitrarily poses too great a risk. Losing up to 90 percent of the content would be unacceptable in this case, and due to structural differences, this becomes impossible beyond certain key points. Moreover, the summaries generated by GPT have sometimes been perceived as better than models specifically designed for this purpose [@goyal_news_2023]. A significant portion of this work involved data collection, which proved to be a major challenge. Due to the lack of access to Compustat, commonly used in financial studies and for SEC-related information, we had to rely on extensive research. Various approaches had to be abandoned as they failed to meet expectations. Nevertheless, we were able to complete our work, albeit with some compromises regarding the amount of data.

In conclusion, the findings of our study indicate that there is a positive correlation between BMI and firm performance, particularly in terms of operating revenue. Nevertheless, it is imperative to acknowledge that the observed correlation does not necessarily imply a causal effect. The intricate nature of organisational structures and the limitations of the current dataset preclude us from drawing definitive conclusions regarding causality. Nevertheless, this study can be regarded as a continuation of the approach taken by Hoberg & Phillips [@hoberg_text-based_2016], and the examination of BMI's impact on performance could be further explored in future studies, offering valuable extensions to this research. Furthermore, our findings suggest that modifications to the BM have a positive impact on revenue growth for small firms. It is essential to avoid misinterpreting the results for larger firms by assuming that such adjustments are unprofitable or illogical. This study primarily focuses on short-term effects, making it more challenging to observe the impact on the products of larger companies. While long-term profitability resulting from innovation is acknowledged, it falls outside the scope of this analysis.

## Robustness Checks

To check the validity and reliability of this study, we conduct a series of robustness checks, illustrating alternative models. The revenue growth and the normalized distance are not logarithmized in this specification. Revenue growth can assume negative values and is a relative measure. The normalized distance is also a relative measure, and the distance itself is characterized by a norm distribution (according to Figure 1). Logarithmization is therefore not required. The use of market value and total assets as control variables makes it possible to control for the size of the company, which results on the one hand from supply and demand on the capital market, and on the other hand from the actual book value of the company. The correlation between these two variables is approximately 0.5. In Table 5, we perform a regression using SIC codes instead of the industry classification based on BERTScores as fixed effects.

```{r echo=FALSE, warning = FALSE, message = FALSE, results='asis'}
#Main Regression Results
library(tidyverse)
library(readxl)
library(lfe)
library(DescTools)
library(stargazer)

RegressionData = read.csv("./TableData/Regression/RegressionData1.csv") %>%
  filter(MV>0, Total.Assets >0)%>%
  mutate(year=as.factor(year),
         sic = as.factor(sic))

RegressionData$RevenueGrowth = Winsorize(RegressionData$RevenueGrowth,probs=c(0,0.99))
RegressionData$TobinsQGrowth = Winsorize(RegressionData$TobinsQGrowth,probs=c(0,0.99))

#Revenue Growth
Reg1 = felm(RevenueGrowth~NormalizedDistance + log(MV) | year * sic | 0 | 0 ,data=RegressionData)
Reg2 = felm(RevenueGrowth~NormalizedDistance + log(Total.Assets) | year * sic | 0 | 0 ,data=RegressionData)

Reg3 = felm(TobinsQGrowth~NormalizedDistance + log(Total.Assets)| year * sic | 0 | 0 ,data=RegressionData)
Reg4 = felm(TobinsQGrowth~NormalizedDistance + log(MV)| year * sic | 0 | 0 ,data=RegressionData)

rows <- tribble(~term, ~Reg1, ~Reg2, ~Reg3, ~Reg4,
                'Industry x Year Fixed Effects', 'Yes', 'Yes', 'Yes', 'Yes')
attr(rows, 'position') <- c(8)

modelsummary(models = list("(1)" = Reg1, "(2)" = Reg2, "(3)" = Reg3, "(4)" = Reg4),
             output = "kableExtra",
             title = "Robustness Check",
             coef_map = c("NormalizedDistance" = "Normalized Distance", "log(MV)" = "log(Market Value)", "log(Total.Assets)" = "log(Assets)"),
             gof_map = c("nobs", "r.squared"),
             stars = TRUE,
             add_rows = rows
             )%>%
  add_header_above(c(" " = 1, "Sales Growth" = 2, "Tobin's Q Growth" = 2)) %>%
  kable_styling(latex_options = c("scale_down","HOLD_position"))%>%
  footnote("Industry classification by SIC codes.")
```

The number of observations decreases due to missing SIC codes for non-surviving companies. As a result Table 5 rather tend to contain a survivor ship bias (the number of observations decreases due to missing SIC codes for non-surviving companies). In addition, the coefficients for sales growth are no longer significant, the standard errors of the coefficients increase despite lower coefficient values. The value for $R^2$ also decreases for each model. This suggests that BERT-based industry classifications as fixed effects likely capture industry-specific endogenous effects, thereby enhancing both the model’s validity and its informative value. With regard to Table 4, other threshold values could also have been selected for a sample split. Each institution (index providers, stock exchanges, financial analysts, etc.) defines its own thresholds, although these do not usually vary greatly. In our study, we use a flat threshold for small-cap and large-cap firms for simplicity, although definitions for very small or very large firms would also require more classifications for a sample split. Due to the limited number of observations, we consider it sensible to dispense with further classifications. 

# Conclusion

The present study builds on the work of Hoberg & Phillips [-@hoberg_text-based_2016] and extends their approach to improving traditional industry classifications by analyzing product descriptions in the context of BM. This research leverages modern methodologies to incorporate entire 10-K filings into the analysis. Unlike Hoberg & Phillips [-@hoberg_text-based_2016], who focus on frequent nouns in a Word-to-Vec model, we employ BERT, which allows us not only to analyze the entirety of the 10-Ks but also to consider their semantic meaning. However, due to BERT’s token length restrictions, we also use the AI tool Gemini to adjust the length of the 10-Ks. To remain consistent with the methodology of the referenced study, we group the BERTScores into industries through hierarchical clustering. We demonstrate that our classification exhibits a distribution similar to the FIC by Hoberg & Phillips [-@hoberg_text-based_2016]. Contrary to our expectations, there is no significant general overlap between the two classification methods.

Beyond extending the industry classification using 10-K filings, we also see further potential in the BERTScores. Therefore, we conducted additional analyses to examine the relationship between BMI and financial performance. Using regression analysis, we find evidence of a correlation: greater changes in the BM are associated with higher revenue growth and a lower Tobin's Q. These results, however, are based on the overall observations, without distinguishing between companies of different sizes. Upon further analysis using multiple regressions segmented by company size, we observe that the effect on revenue growth is statistically significant only for smaller firms.

In conclusion, we find that smaller firms benefit more from BMI, or in the short term, are the only ones that show a positive impact. Additionally, the insights and methodology presented in this study can be applied to discover new effects of BMI. For instance, an increase in R&D spending does not always result in innovation, a distinction that can be better determined using our new approach.

# Acknowledgement

The authors acknowledge support by the state of Baden-Württemberg through bwHPC. Additionally, we extend our sincere thanks to Prof. Dr. Sebastian Kranz for providing the GitHub repository and Jonathan Wunden for his assistance in working with bwHPC.

{{< pagebreak >}}

# References

::: {#refs}
:::

{{< pagebreak >}}

# Appendix

## Appendix A

We used following prompt: "Summarize the business model from the following text. Answer with a continuous text and with five hundred twelve tokens at max. Set your focus on sources of revenue, the intended customer base, products, distribution channels and details of financing. Use only information from the following the text".[^4] "Intended customer base" and "product" refer to the value offering, "distribution channels" refers to the value architecture, and "sources of revenue" and "details of financing" refer to the revenue model. The term 'tokens' was used deliberately in preference to 'words', given that the number of tokens and the number of words in a text may vary depending on the tokeniser. This way, we wanted to ensure that the whole summary is used by the BERT model.
Table 6 presents the descriptive measures of the length of our summaries. On average our

```{r echo=FALSE, warning = FALSE, message = FALSE, results='asis'}
library(tidyverse)
library(knitr)
library(kableExtra)
library(readxl)

Table6 = read_excel("./TableData/Table6.xlsx")%>%
  select(-c("...1"))%>%
  mutate(average = round(average,0),
         standard_deviation= round(standard_deviation,0),
         `25%_quantil` = round(`25%_quantil`,0),
         `75%_quantil` = round(`75%_quantil`,0))%>%
    kable(format="latex",label="tbl-table5",col.names = c("Report-for Year","Average Word Count","Standard Deviation","Minimum","25th Percentile","Median","75th Percentile","Maximum"),booktabs = T, caption = "Descriptive Statistics of Number of Words of our Summaries")%>%
  kable_styling(latex_options = c("scale_down","HOLD_position"))%>%
  footnote("All 21,417 summaries were considered.")

Table6
```

summaries are 350 till 371 words long with the 75th percentile ranging between 404 and 432 words. Looking at the maxima, some summaries are significantly longer than the average with a couple thousand words. Most of the summaries are short enough to be processed by BERT as a whole, while some summaries are too long for BERT. These outliners are only a few and therefore negligible.

[^4]: The spelling error in the last sentence of the prompt was found after processing Item 1. After evaluating the summaries, this error did not cause any issues.

{{< pagebreak >}}

## Appendix B

To check the validity of the summaries generated by Gemini, we took a random sample of ten filings to evaluate them manually. The results are shown in Table 7.

```{r echo=FALSE, warning = FALSE, message = FALSE, results='asis'}
Table7 <- read_excel("./TableData/Table7.xlsx") %>%
kable(format="latex",label="tbl-table6",col.names = c("Accession ID", "Company Name","Year"),booktabs = T, caption = "Subjective Evaluation of Random Item 1 Summaries")%>%
  kable_styling(latex_options = c("scale_down","HOLD_position"))%>%
  footnote("A random sample of ten filings was drawn")
Table7
```

We find that the summaries are valid overall and consistently reflect the BM. Suitable representations of all Item 1 texts can be found in all corresponding summaries available in the sample. We therefore assume that the summaries are a valid representation of the original company descriptions. One conspicuous feature was that although strong diversification tends to be mentioned in the summaries, no possible interaction between these areas is discussed.

{{< pagebreak >}}

## Appendix C

In our second dataset we winsorize only the top quantile. Figure 3 presents a boxplot of the distribution of the sales growth

```{r echo=FALSE, warning = FALSE, message = FALSE, results='asis'}
#| label: fig-3
#| echo: false
#| fig-cap: "Distribution of the Sales Growth"
#Boxplot
library(tidyverse)
library(ggplot2)
library(readxl)


RegressionData = read.csv("./TableData/Regression/RegressionData.csv")

Figure3 = RegressionData %>%
  ggplot(aes(y=RevenueGrowth,x=1))+
  geom_boxplot()+
  theme_minimal()+
    theme(axis.title.x=element_blank(),
          axis.text.x=element_blank())+
    scale_y_log10(breaks=c(0.1,1,10,100,1000,10000,100000,1000000,10000000),labels=c("0.1%","1%","10%","100%","1,000%","10,000%","100,000%","1,000,000%","10,000,000%"))+
  ylab("Sales Growth in Percent")

Figure3
```

and also explains our reasoning behind this decision. The boxplot shows that the big majority of observations has a growth rate below 1,000 percent. Sales growth rates of multiple ten thousand percent or even of a couple million percent seem very unrealistic and are probably due to poor data quality. By winsorizing at the top one percent quantile, we get ride of these 

```{r echo=FALSE, warning = FALSE, message = FALSE, results='asis'}
#Appendix
library(tidyverse)
library(readxl)
library(lfe)
library(DescTools)
library(stargazer)

RegressionData = read.csv("./TableData/Regression/RegressionData.csv") %>%
  filter(MV>0, Total.Assets >0)%>%
  mutate(year=as.factor(year),
         Industry = as.factor(Industry))

#Revenue Growth
Reg1 = felm(RevenueGrowth~NormalizedDistance + log(MV) | year * Industry | 0 | 0 ,data=RegressionData)
Reg2 = felm(RevenueGrowth~NormalizedDistance + log(Total.Assets) | year * Industry | 0 | 0 ,data=RegressionData)

rows <- tribble(~term, ~Reg1, ~Reg2,
                'Industry x Year Fixed Effects', 'Yes', 'Yes')
attr(rows, 'position') <- c(8)

modelsummary(models = list("(1)" = Reg1, "(2)" = Reg2),
             output = "kableExtra",
             title = "Regression Results (Not Winsorized)",
             coef_map = c("NormalizedDistance" = "Normalized Distance", "log(MV)" = "log(Market Value)", "log(Total.Assets)" = "log(Assets)"),
             gof_map = c("nobs", "r.squared","rmse"),
             stars = TRUE,
             add_rows = rows
             )%>%
  add_header_above(c(" " = 1, "Sales Growth" = 2)) %>%
  kable_styling(latex_options = c("scale_down","HOLD_position"))%>%
  footnote("This is the same estimation as in Table 3, but without winsorizing the data.")
```

unrealistically high values. For the sake of completeness, Table 8 reports the regression results without winsorizing the data. All coefficients are not significant anymore while their values and their standard errors increase massively.

{{< pagebreak >}}

## Appendix D


In this section, we aim to explore whether differences in revenue growth can be attributed to varying levels of innovation strength across different company sizes.


```{r echo=FALSE, warning = FALSE, message = FALSE, results='asis'}
#| label: fig-1-A
#| echo: false
#| fig-cap: "Density Function of the Distance for every Cap-Size"

library(ggpattern)

RegressionData <- read.csv("./TableData/Regression/RegressionData.csv")

Figure1_2_dat <- RegressionData %>%
 mutate(FirmSize = case_when(
  MV < 1000 ~ "Small_Cap", 
   MV >= 1000 & MV < 5000 ~ "Mid_Cap", 
   TRUE ~ "Large_Cap"
 ))


desk_dat <- Figure1_2_dat%>%
  group_by(FirmSize)%>%
  summarise(Mean=1-mean(Score))

Figure1_2 = Figure1_2_dat%>%
  ggplot(aes(x=1-Score, fill = FirmSize, color = FirmSize)) +
  geom_density(alpha = 0.25) +
  labs(caption= "Note:\nOnly distances for same company between different years.",x = "Distance", y = "Density") +
  # geom_vline(xintercept=desk_dat$Mean[desk_dat$FirmSize=="Small_Cap"],alpha=0.5)+
  geom_tile(aes(x = 0.2, y = 15), fill = "#619CFF", width = 0.062, height = 0.9, color = "#619CFF") + 
  geom_tile(aes(x = 0.2, y = 14), fill = "#00BA38", width = 0.062, height = 0.9, color = "#00BA38") + 
  geom_tile(aes(x = 0.2, y = 13), fill = "#F8766D", width = 0.062, height = 0.9, color = "#F8766D") +   
  annotate("text", x=0.18, y=15, label=paste("Mean Small Caps: ", round(desk_dat$Mean[desk_dat$FirmSize=="Small_Cap"], 4)), angle=0, size= 4)+
  # geom_vline(xintercept=desk_dat$Mean[desk_dat$FirmSize=="Mid_Cap"],alpha=0.5)+
  annotate("text", x=0.18, y=14, label=paste("Mean Mid Caps: ", round(desk_dat$Mean[desk_dat$FirmSize=="Mid_Cap"], 4)), angle=0, size= 4)+
  # geom_vline(xintercept=desk_dat$Mean[desk_dat$FirmSize=="Large_Cap"],alpha=0.5)+
  annotate("text", x=0.18, y=13, label=paste("Mean Large caps: ", round(desk_dat$Mean[desk_dat$FirmSize=="Large_Cap"], 4)), angle=0, size= 4)+
  scale_fill_manual(
    values = c("Small_Cap" = "#F8766D", "Mid_Cap" = "#00BA38", "Large_Cap" = "#619CFF") )+
  scale_color_manual(
    values = c("Small_Cap" = "#F8766D", "Mid_Cap" = "#00BA38", "Large_Cap" = "#619CFF") )+
  #scale_fill_discrete(name = "Firm Size", labels = c("Large Cap", "Mid Cap", "Small Cap"))#+
    theme_minimal()
  

Figure1_2

```

The figure presents three density distributions, each corresponding to a specific company size. Red represents data for small companies, green medium-sized firms, and blue shows the large companies. As noted in the Methodology chapter, these distributions appear to follow a normal distribution. There are subtle differences between the company sizes, with the small firms' distribution slightly shifted to the right. Additionally, the distribution for large companies is more concentrated around the mean, while the medium-sized firms show a broader distribution. Overall, the three densities look quite similar at first glance, making it difficult to draw definitive conclusions about the significance of these differences. In the top right corner of the chart, the mean values for each company size are displayed. This shows that the mean values for medium and large firms are very close, while the mean for small companies is slightly higher, which aligns with the previously mentioned rightward shift.

```{r echo=FALSE, warning = FALSE, message = FALSE, results='asis'}

# test if the difference is significant

dat_reg_sig <- Figure1_2_dat%>%
    mutate(year=as.factor(year),
         Industry = as.factor(Industry))

dat_reg_sig$FirmSize <- factor(dat_reg_sig$FirmSize, levels = c("Small_Cap", "Mid_Cap", "Large_Cap"))

reg_sig_comp <- lm(formula =  distance ~ FirmSize, data = dat_reg_sig)

reg_sig_comp_fe <- felm(formula =  distance ~ FirmSize | year*Industry | 0 | 0 , data = dat_reg_sig)


rows <- tribble(~term, ~reg_sig_comp, ~reg_sig_comp_fe,
                'Industry x Year Fixed Effects', 'No','Yes')
attr(rows, 'position') <- c(5)

modelsummary(models = list("all" = reg_sig_comp, "fe" = reg_sig_comp_fe),
             output = "kableExtra",
             title = "Regression Results: Difference of Distance",
             coef_map = c("FirmSizeMid_Cap" = "Mid_Cap_Firm", "FirmSizeLarge_Cap" = "Large_Cap_Firm"),
             gof_map = c("nobs", "r.squared","rmse"),
             stars = TRUE,
             add_rows = rows
             )%>%
  add_header_above(c(" " = 1, "Distance" = 2))%>%
  kable_styling(latex_options = c("scale_down","HOLD_position"))

```

Due to the observed differences in mean values, we conducted two additional regression analyses to test the significance of the distance differences. In these regressions, Distance was used as the dependent variable and FirmSize as the independent variable, with small companies serving as the reference category. The results of the first regression indicate that the differences are statistically significant, suggesting that small companies engage in more innovation than medium and large firms. However, these results are not highly robust, as all companies were compared without considering fixed effects.

To address this, we conducted the same regression, this time incorporating fixed effects for year and industry. The results of this adjusted regression suggest that company size is not a significant factor, thereby challenging the previous findings. This indicates that companies, regardless of size, innovate in a manner consistent with their industry, suggesting that innovation levels are not necessarily dependent on company size.


