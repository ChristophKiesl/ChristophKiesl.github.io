% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
]{article}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
    \setmainfont[]{Latin Modern Roman}
  \setmathfont[]{Latin Modern Math}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\usepackage{siunitx}

  \newcolumntype{d}{S[
    input-open-uncertainty=,
    input-close-uncertainty=,
    parse-numbers = false,
    table-align-text-pre=false,
    table-align-text-post=false
  ]}
  
\usepackage{arxiv}
\usepackage{orcidlink}
\usepackage{amsmath}
\usepackage[T1]{fontenc}
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={From Text to Insight - A Novel Approach to Measuring Business Model Innovation},
  pdfauthor={Max Gabler; Wanshu Jiang; Christoph Kiesl; Leonard Pöhls; Alexander Rieber; Ansgar Scherp},
  pdfkeywords={10-K, Business Model Innovation, BERT, Gemini},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\newcommand{\runninghead}{A Preprint }
\title{From Text to Insight - A Novel Approach to Measuring Business
Model Innovation}
\def\asep{\\\\\\ } % default: all authors on same column
\author{\textbf{Max
Gabler}\\\href{mailto:max.gabler@uni-ulm.de}{max.gabler@uni-ulm.de}\asep\textbf{Wanshu
Jiang}\\\href{mailto:wanshu.jiang@uni-ulm.de}{wanshu.jiang@uni-ulm.de}\asep\textbf{Christoph
Kiesl}\\\href{mailto:christoph.kiesl@uni-ulm.de}{christoph.kiesl@uni-ulm.de}\asep\textbf{Leonard
Pöhls}\\\href{mailto:leonard.poehls@uni-ulm.de}{leonard.poehls@uni-ulm.de}\asep\textbf{Alexander
Rieber}\\\href{mailto:alexander.rieber@uni-ulm.de}{alexander.rieber@uni-ulm.de}\asep\textbf{Ansgar
Scherp}\\\href{mailto:ansgar.scherp@uni-ulm.de}{ansgar.scherp@uni-ulm.de}}
\date{}
\begin{document}
\maketitle
\begin{abstract}
The ability of a company to continuously innovate its business model is
a pivotal determinant of long-term success in dynamic markets. It is
therefore crucial to ensure the reliability of business model innovation
measurement. In this study, we utilize business descriptions from 10-K
filings between 2017 and 2023, which have been summarized using the
Gemini language model, to measure business model innovation. We find
that above-market business model innovation, which we measure by the
annual change in business description, has a positive effect on a firm's
sales growth but a negative effect on its Tobin's Q growth.
Additionally, we find that small-cap companies in particular can expect
sales growth from business model innovation. These findings offer
insights into the extent to which textual similarities in regulatory
reports can be employed as a reliable indicator for business model
innovation. Thus, this method represent a novel approach to analyzing
business model innovation over time.
\end{abstract}
{\bfseries \emph Keywords}
\def\sep{\textbullet\ }
10-K \sep Business Model Innovation \sep BERT \sep 
Gemini


\newpage{}

\section{Introduction}\label{introduction}

Business model innovation (BMI) is a key activity to maintain
competitiveness and even gain a competitive advantage in todays fast
paced markets (Pucihar et al. 2019; Teece 2018). It is therefore no
surprise that the interest in BMI has grown rapidly over the last twenty
years. In particular, research examining the impact of BMI on firm
performance has been a prominent area of investigation, with numerous
research papers published in this field (Cucculelli and Bettinelli 2015;
Latifi, Nikou, and Bouwman 2021; Zott and Amit 2008; White et al. 2022).
While the financial literature offers a wide range of established
methods for measuring a company's performance, the BMI literature
provides only a limited number of measures, all of which face similar
challenges (White et al. 2022). Furthermore, these measures vary
largely. In order to further validate and advance the BMI research
field, more sophisticated and comprehensive measurement instruments are
necessary (Huang and Ichikohji 2023).

Scales and measures used in the BMI literature (Clauss 2017; Spieth and
Schneider 2016) provide managers and practitioners with a measurement
index for BMI. But these measures only validate applicability of BMI
theory (Huang and Ichikohji 2023) and are insufficient for longitudinal
studies (Clauss 2017). Hence, these measures are not adequate for a time
series analysis of BMI. We address this gap by proposing a novel
approach to measuring BMI. US-based companies are obliged by the United
States Security and Exchange Commission (SEC) to submit annual 10-K
filings, wherein a detailed description of the company's business
operations is required. Hoberg \& Phillips (2016), on which we build
this study, use these filings to cluster companies into industries. We,
on the other hand, summarize these descriptions with Gemini and
calculate the BERTScore between the summaries of different years for a
single company. This approach enables the measurement of changes in the
business model (BM) over time as the distance between the BM summary of
one year to another. In order to test the validity of our measure, we
regress sales growth and Tobin's Q growth on our measure. Additionally,
we create our own industry classification based on BERTScores of
business descriptions of same firms within the same year.

We conduct multivariate regressions and find that our measure is
positively correlated with firm performance, showing its validity.
Furthermore, we find that BMI has a greater impact on firm performance
for smaller companies than medium and large companies. Consequently, we
contribute to the literature by introducing a novel approach of
measuring BMI, which overcomes drawbacks from existing approaches. Our
measure and approach enable researchers to study BMI in longitudinal and
large-scale studies. Lastly, we introduce our own industry
classification based on a company's BM, which represents an alternative
to existing ones.

The SEC mandates that the majority of public companies based in the
United States submit an annual 10-K filing. In the first section a
company presents its general business, encompassing information about
its products and services. In some instances additional topics may be
addressed, such as labor issues or competition (SEC 2024). In
conclusion, this section contains the most useful information for
describing a company's BM (Lee and Hong 2014). Furthermore, 10-K filings
are a reliable source of information, given that US law prohibits false
or misleading statements in the filings. The SEC monitors the compliance
of the companies with the requirements and comments where disclosure
appears to be inconsistent (SEC 2024). Because of these guidelines,
these descriptions are particularly suitable for a text analysis in
order to quantify a BM.

Despite the growing interest in BMI and the increasing number of
theoretical and empirical studies in this field, the research of BMI is
still in a preliminary state (Huang and Ichikohji 2023). Consequently,
there is considerable variation in the definitions of BMI, with some
definitions being more similar to one another than others (Foss and
Saebi 2017). Spieth \& Schneider (2016) identify three core dimensions
that comprise a company's BM: its value proposition, its value creation
architecture and its revenue model logic. Based on this, BMI can be
conceptualized as a change that is new-to-the-firm in at least one of
these dimensions. Furthermore, Spieth and Schneider (2016) introduce a
measurement model to evaluate these three dimensions of BMI. They
develop an index by first specifying the contents, followed by a
specification of the indicators and assessing their content validity,
assessing the indicators collinearity and finally assessing the external
validity. Clauss (2017) employs a very similar approach. After
specifying the domain and dimensionality of BMI through literature
research, the author divides his scale into three hierarchical levels,
which are very similar to the ones of Spieth and Schneider (2016). We
build on these conceptualizations to design our prompt we use in the
pre-processing with Gemini. However, both measures are subject to three
significant limitations. Firstly, both measures lack a temporal
component. Consequently, they are inadequate for use in longitudinal
studies or ex-post evaluations of BMI. Secondly, BMI is only measured at
the new-to-the-firm level rather than at the new-to-the-industry or
new-to-the-market level. Thirdly, both measures rely on interviews and
questionnaires, which makes conducting large-scale studies
time-consuming and reliant on the willingness of the companies to
cooperate (Clauss 2017; Spieth and Schneider 2016). Our novel
measurement approach tackles the first and third issue.

A number of studies have examined the relationship between BMI and the
financial performance of a company. Cucculelli \& Bettinelli (2015)
investigate the effect of BMI on sales growth, return on sales (ROS) and
total factor productivity (TFP). The results support the hypothesis that
BMI has a positive effect on firm performance, with the effect
increasing in line with the intensity of the innovation. Desyllas et al.
(2022) find that BMI has a small effect on performance of incumbent
firms. They measure firm performance by Tobin's Q growth. White et al.
(2022) conducted a meta-analysis based on the extant BMI literature.
They found a positive relationship between BMI and firm performance, and
that this relationship is shaped by factors including the firm age,
industry, the economic and political environment and BMI
characteristics. Based on this, we derive the dependent and control
variables in the estimation strategy.

Hoberg \& Phillips (2016) present a novel approach to defining industry
boundaries. They propose two novel industry classification methods: the
fixed industry classification (FIC) and the text-based network industry
classification (TNIC). Firstly, they cluster companies based on the
similarity of word vectors into fixed industries. Secondly, they define
a minimum similarity threshold, above which firms are considered in the
same industry. This second step relaxes their prior properties of binary
membership transitivity and fixed industry location. The authors
demonstrate shortcomings in the traditional industry classification
systems such as the Standard Industry Classification (SIC) and the North
American Industry Classification System (NAICS), which are not able to
account for temporal changes. The new method is capable of capturing
changes in industry boundaries and competitor sets over time, thereby
providing a dynamic industry classification system. Based on the FIC we
propose our own BERTScores industry classification and utilize it in our
estimation. In their study, Lee \& Hong (2014) examine the evolution of
a firm's BM over time. After filtering the Item 1 parts of the 10-K
filings for relevant sentences, Lee \& Hong (2014) construct keyword
vectors, which represent the concept of the BM. Therefore, the evolution
of the BM is depicted as the change in the distribution of keywords over
time. The authors advocate for a more robust methodology, such as
incorporating multi-word phrases in the keyword vectors, to enhance the
reliability of the approach (Lee and Hong 2014). Our study pursues a
similar goal but with a novel methodology.

The remainder of the paper is organized as follows. Section 2 describes
the origin and preparation of our data, the use and functioning of BERT,
the preprocessing with Gemini and our methodology. Section 3 lays out
our estimation strategy. Section 4 contains our results and discussion.
Section 5 concludes our study.

\section{Data and Methodology}\label{data-and-methodology}

\subsection{The Dataset}\label{the-dataset}

We collect 10-K filings from the digital SEC Database, using the
category ``10-K'' as extraction condition. Since the focus of our study
lies on company's BM, we merely use the Item 1 part, since this is the
most crucial part of the 10-K filings for describing the companies BM
(Lee and Hong 2014). Our observations are limited to an intersection of
such companies, which has been made available to the SEC since 2001 in a
publicly accessible database. We extracted 10-K filings that were
submitted between 2017 and 2023 based on underlying Central Index Keys
(CIK). Occasionally, such filings are submitted retrospectively or are
already submitted for the same year. We are therefore limiting the
period for which we are reporting to 2016-2023, with fewer observations
available for 2016 and 2023 as a result. We exclude companies from the
financial sector, namely companies with a SIC Code starting with six.
Corresponding to Table 2, multiple steps of pre-processing were required
to obtain the final amount of 21,417 observations for seven years.
Financial key figures, including sales, total assets, market values and
Tobin's Q were extracted from DataStream. A total of 4,494 companies are
included in the sample, although the availability of filings could not
always be guaranteed for all years. This is due on the one hand to the
quality of the API to the SEC and on the other hand to companies that
did not file 10-K reports or were listed on the stock exchange for the
entire period under review. Finally, we have access to the financial key
figures of the companies for the respective year, the Item I text
pre-processed with the help of Gemini, company-specific identification
features and the conventional SIC industry classification. The
information on SIC sector classification is limited to companies that
are currently actively filing. Therefore, 609 firms are no longer
actively filing, e.g.~due to company bankruptcies or mergers.

\begin{table}[H]
\centering
\caption{\label{tab:tbl-table1}10-K Sample Creation}
\centering
\resizebox{\ifdim\width>\linewidth\linewidth\else\width\fi}{!}{
\begin{tabu} to \linewidth {>{\raggedright\arraybackslash}p{8cm}>{\raggedright}X>{\raggedright}X}
\toprule
Source/Filter & Sample Size & Observations Removed\\
\midrule
1. Original (exchanged listed) companies, whose 10-K filings are extracted from SEC & 47,226 & 0\\
2. Removing observations from financial companies whose SIC-code start with '6' & 37,750 & 9,476\\
3. Verify for Item 1 text availability (removed oberservations that are attributable to API quality) & 32,611 & 5,139\\
4. Extracting dates for which the filings are reporting for and removing of duplicated filings & 30,737 & 1,874\\
5. Delete observations with incorrect date assignment (some companies submitted two or more filings due to addendums or data quality) & 30,131 & 606\\
\addlinespace
6. Merged Gemini processed Item 1 text to the underlying data set. We did not consider texts that were not processable (e.g. due to recitation errors) & 28,350 & 1,781\\
7. Extract financials statements from DataStream and merge them. Also remove observations for report-for-years prior to 2016 & 21,417 & 6,933\\
\bottomrule
\multicolumn{3}{l}{\rule{0pt}{1em}\textit{Note: }}\\
\multicolumn{3}{l}{\rule{0pt}{1em}Filings submitted between 2017 and 2023 are considered.}\\
\end{tabu}}
\end{table}

\subsection{BERT and BERTScore}\label{bert-and-bertscore}

BERT is a pre-trained and transformer-based model for natural language
processing (NLP) based on artificial neural networks. It works according
to the transformer architecture, first mentioned by Vaswani et al.
(2017). Unlike Hoberg \& Philips' (2016) word-to-vec approach, BERT
operates bidirectionally, considering the context from both sides of
each word simultaneously. This bidirectional design helps clarify word
meanings based on context, resulting in more accurate similarity
calculations between texts that may use the same words but convey
different meanings. BERT can also be fine-tuned for specific tasks,
making it adaptable to different datasets, improving its performance
even with limited labeled data. Thus, BERT effectively captures deeper
semantics in texts such as 10-K reports. The BERTScore, a metric built
on BERT embeddings, computes cosine similarity between word or text
meanings learned by BERT, where a scale from -1 to 1 is used, with 1
representing perfect similarity.

In our study, we employ the `bert-base-uncased' model in two distinct
but complementary tasks. First, for inter-company comparisons, we use
BERT to generate embeddings for product descriptions from different
companies by processing each entry. The BERT tokenizer converts input
text into tensors, and the model generates embeddings by averaging the
token embeddings from the last hidden state. We finally compute pairwise
cosine similarity scores between them to assess semantic similarity
across companies' product descriptions. In parallel, we apply a similar
approach to assess year-to-year evolution of product descriptions within
individual companies. For each company (identified by its CIK), we
retrieve product descriptions from two consecutive years and generate
embeddings using the same BERT model. Here, we utilize the BERTScore
metric to compare descriptions from adjacent years, such as 2017 and
2018. The similarity is calculated based on precision (P), recall (R),
and F1-score, with the F1-score serving as the primary measure. This
year-to-year comparison enables us to analyze how product descriptions
evolve over time.

\subsection{Preprocessing with Gemini}\label{preprocessing-with-gemini}

10-K filings are typically very large text documents, and Item 1 of
these filings is no exception. Table 2 shows the descriptive measures of
the length of the original Item 1 section in our final sample. The
length of a document was measured by the word count without punctuation.
The document length ranges from a minimum if 49 words up to 78,799
words. On average the documents are between 6,626 and 10,304 words long.
In order to utilize the entirety of the information regarding the BM in
the Item 1 section and pass the text to our BERT model, we decided to
let Google's GenAI chatbot Gemini summarize them to a maximum length of
512 tokens. The summaries were created between 26 June 2024 and 6 August
2024. The model employed was Gemini Flash 1.5. The prompt was inserted
at the beginning of each text file and it was passed via an API to
Gemini \footnote{We forked and used following Github repository:
  https://github.com/skranz/gemini\_ex.}. Our prompt covers all aspects
of the definition of BMI proposed by Spieth \& Schneider (2016). For
more details, see Appendix A. To assess the quality and accuracy of the
summaries produced by Gemini, a random sample of 100 filings was
selected for comparison with the original text. More precise,the
original file was initially read with a focus on the points mentioned in
the prompt. Subsequently, the summary was evaluated to ascertain whether
it contained these same points. A list of the sample with the summaries
is provided in the Appendix B.

\begin{table}[H]
\centering
\caption{Descriptive Statistics of Number of Words in Original Filings}
\centering
\resizebox{\ifdim\width>\linewidth\linewidth\else\width\fi}{!}{
\begin{tabular}[t]{rlllllll}
\toprule
Report-for Year & Average Word Count & Standard Deviation & Minimum & 25th Percentile & Median & 75th Percentile & Maximum\\
\midrule
2016 & 7,765 & 6,053 & 134 & 3,658 & 5,972 & 10,251 & 51,227\\
2017 & 7,426 & 6,162 & 134 & 3,474 & 5,689 & 9,538 & 70,611\\
2018 & 7,552 & 6,290 & 49 & 3,492 & 5,738 & 9,599 & 74,351\\
2019 & 7,943 & 6,562 & 59 & 3,646 & 5,961 & 10,241 & 78,270\\
2020 & 8,610 & 7,157 & 59 & 3,930 & 6,518 & 10,914 & 57,980\\
\addlinespace
2021 & 10,304 & 8,347 & 235 & 4,671 & 7,614 & 13,637 & 78,799\\
2022 & 9,478 & 7,977 & 57 & 4,321 & 7,057 & 11,919 & 73,937\\
2023 & 6,626 & 4,750 & 190 & 3,667 & 5,807 & 8,366 & 43,523\\
\bottomrule
\multicolumn{8}{l}{\rule{0pt}{1em}\textit{Note: }}\\
\multicolumn{8}{l}{\rule{0pt}{1em}All 21,417 original filings were considered.}\\
\end{tabular}}
\end{table}

\subsection{Methodology}\label{methodology}

From the dataset containing all the summaries and financials per company
per year we construct three datasets for our further analysis. The first
dataset is used for the BERTScore industry classification. Therefore, we
filter for all summaries of 2017 and for the first available summary of
companies which do not appear first in 2017 but in a later year. We then
calculate the BERTScores between all these summaries. The second and
third dataset are used for the regression analysis of our measure. For
the second dataset, we fix the company and calculate the BERTScore of
the summaries of one year and its direct consecutive year as well as the
sales growth rate. The third dataset contains the Tobin's Q growth
calculated as the difference between the natural logarithm of Tobin's Q
in the last and the first year of our observed period as well as the
BERTScore between the summaries of the respective year of a company.

The first dataset is utilized to compute our BERTScore industry
classification. We firstly compare the BERTScore industry classification
with the FIC by Hoberg \& Phillips (2016) and the SIC. Furthermore, we
use our BERTScore industry classification in our estimation. For the
FIC, Hoberg \& Phillips (2016) calculate the cosine similarity between
word vectors of product descriptions, which they extracted from Item 1
of the 10-K filings. For our Industry classification we utilize the
BERTScore to calculate the similarity between our BM summaries. Based on
these similarities we cluster the companies into industries via an
agglomerative clustering algorithm. The methodology and object of
research differ between the two studies. In accordance with the
definition provided by Spieth \& Schneider (2016), a company's product
constitutes a component of its value proposition and, consequently, a
constituent of the BM. Because the product is thereby entangled with the
BM, companies that have similar products might have similar BMs. So
despite the different methodology and object of research, we expect a
similar distribution as Hoberg for the FIC, which is very granular and
contains lot of small industries. Thus, we hypothesize:

\textbf{H1}: Our BERTScore industry classification shows a similar
distribution compared to the FIC.

\textbf{H2}: Our BERTScore industry classification has a high overlap
with the FIC.

As mentioned, our approach differs from the original paper by Hoberg \&
Phillips (2016). We fix the company and calculate the BERTScore of the
summaries of one year and the following year. When a company innovates
its BM over time, the 10-K filings change and thus the summaries of
these filings. We subtract the BERTScores from one to get the distance
instead of the similarity between summaries, because the distance yields
a more intuitive interpretation: The higher the distance, the more do
the BM summaries differ. Figure 1 shows the density function of the
distance. The distance looks normally distributed and has a mean of
0.104 and a standard deviation of 0.027. The values range from zero to
0.216. This means that on average the summaries of a company differ
slightly. We attribute these small differences to our preprocessing
rather than that companies on average slightly change their BM every
year. Even if the summaries are very similar in terms of content, Gemini
might use different phrases and wordings which result in different
BERTScores and thereby in a higher distance. Hence, we normalize the
distance by dividing by its mean. Furthermore, we subtract one and
multiply it by one-hundred in order to further ease the interpretation
of the coefficient. This results in the following definition of our
measure:

\[
{NormalizedDistance} = \left( \frac{{Distance}_i}{\frac{1}{n} \sum_{i=1}^{n} {Distance}_i} - 1 \right) \cdot 100
\]

where n denotes the number of observations in the dataset. The
Normalized Distance measures the difference in the distance between two
consecutive summaries of some company above the market average in
percentage points. Under the assumption that the summaries reliably
represent the firms' BM, the Normalized Distance measures the change of
a company's BM above the market average in percentage points from one
year to the next. We further assume that a company that changes its BM,
will not return back to its original BM. Under this assumption the
Normalized Distance measures BMI on the new-to-the-firm level. The
literature posits that changes in the business model can be positively
correlated with firm performance. In the case that our measure is able
to measure BMI, we expect to find a positive relationship between our
BMI measure and firm performance. Therefore, we hypothesize that:

\textbf{H3}: Our measure for BMI is positively correlated with firm
performance.

When measuring BMI, Guldmann \& Huulgaard (2020) assume several barriers
to BMI in the circular economy, comparing differences between company
sizes and equal company sizes. They found that even same-sized companies
face different barriers to BMI, but also identify company size, industry
and customer segments as drivers of differences in barriers to BMI.
There is a gap in the literature when it comes to investigating the
effect of BMI on the performance of different company sizes. We expect
higher effects for companies with lower market capitalization (according
to Guldmann \& Huulgaard (2020), smaller companies are more flexible in
changing established processes), while companies with medium or high
market capitalization tend to show a less strong effect, which is why we
put forward the following hypothesis:

\textbf{H4}: BMI has a stronger positive effect on the performance of
small companies compared to medium and large companies.

\begin{figure}

\centering{

\includegraphics{ProjectEcoDataScience_files/figure-pdf/fig-1-1.pdf}

}

\caption{\label{fig-1}Density Function of the Distance}

\end{figure}%

\section{Estimation Strategy}\label{estimation-strategy}

We test H3 using multivariate regression techniques. The independent
variable is the Normalized Distance, as previously defined. Two
different dependent variables are employed in order to measure a
company's performance. In the initial specification, the sales growth is
used as the dependent variable as by Cucculelli \& Bettinelli (2015).
For this specification, the second dataset is employed, and all
observations with zero values for market value and total assets were
removed in order to take the logarithms. We control for firm size by
once using the logarithm of the market value and once the logarithm of
total assets (Dang, Li, and Yang 2018). Moreover, outliners are
winsorized only above the 99th percentile. For a more detailed
discussion of this step, see Appendix C. We use the interaction term of
the year and our BERTScore industry classification as a fixed effect.
The interaction term of the year and the industry is employed to capture
potential heterogeneous effects of the industry classification across
different years. The economic and political environment is not
controlled for, as White et al. (2022) suggest, as all of the companies
in question are based in the United States.

In the second specification the growth of Tobin's Q is employed as
described by Desyllas et al. (2022). To account for differences between
companies, we us growth rates instead of absolute values. The third
dataset is used for this specification. Since every company is one
observation in the dataset, we control for the firm size once with the
logarithm of the mean market value and once with the logarithm of the
mean total assets of a company. The mean is calculated with all
available years per company. Furthermore, the number of years between
the initial and final years utilized for Tobin's Q growth is controlled
for. The reason behind this is, that some companies in the dataset
initially appear later than 2017 or lastly do not appear in 2023. We use
fixed effects to control for this difference. Furthermore, we use
industry fixed effects to control for heterogeneous effects between
industries. As in the first specification, we do not control for the
economic and political environment.

We use a similar multivariate regression model to test H4. In contrast
to the previous specification, we only use sales growth as the dependent
variable, as we would otherwise have too few observations available for
our model due to the sample split. We divide the dataset into three
different groups, using a flat split of market capitalization defined by
NASDAQ as the thresholds. Accordingly, companies with a market
capitalization of up to one billion US dollars are assigned to small-cap
firms, companies up to five billion US dollars to mid-cap firms and
companies larger than that to large-cap firms. As in the first
specification, we use the interaction term of the year and our BERTScore
industry classification as a fixed effect and control for firm size via
the logarithm of the total assets.

\section{Results and Discussion}\label{results-and-discussion}

\subsection{Comparison}\label{comparison}

Our study builds on the idea of Hoberg \& Phillips (2016) to utilize
text data from 10-K filings to classify companies based on their product
similarity into dynamic industries. They achieve this through the
parsing of the product descriptions provided by Item 1 of firms 10-K
filings and creating word vectors. Specifically, the authors identify
and exclude proper nouns, which include common words and geographic
locations. They then create word vectors for each firm and year, which
enables the measurement of product similarity over time. They perform
two steps to create the FIC. Firstly, a hierarchical agglomerative
clustering algorithm is employed to cluster companies based on their
similarity and maximize ex-post within cluster similarity. This enables
a classification with any number of clusters. In the second step, the
authors compute aggregated word vectors for each industry. These vectors
now represent the industries. Subsequently, the similarity between
industries and firms is calculated for each of the following years. From
the second year onwards, firms are classified according to the industry
with which they are most similar. Our approach differs in two ways:
Firstly, in contrast to the TNIC and FIC, which employ word-to-vec, our
approach utilizes BERT to represent text, which allows us to capture the
context of words. Accordingly, the BERTScore is employed instead of the
cosine similarity as our similarity measure. Secondly, our analysis is
focused on the description of the BM rather than on the product
descriptions. Nevertheless, in the following subsection, the BERTScore
industry classification is compared with the FIC and the SIC.

We employ the first dataset for the BERTScore industry classification
and the comparison. The SIC codes come from the SEC website\footnote{The
  list can be found here:
  https://www.sec.gov/search-filings/standard-industrial-classification-sic-code-list.}.
For the FIC we have utilized the similarity scores provided by
Hoberg-Phillips Data Library\footnote{For the database see:
  https://hobergphillips.tuck.dartmouth.edu.}. The data consists of the
gvkeys of two companies, the year and the cosine similarity between
these two companies. In order to ensure comparability, only companies
present in both the present study's dataset and that provided by the
authors are included in the analysis. Because we use CIKs and accession
numbers to identify firms and filings, and the fact that the data
library employs Compustat's gvkeys, the matching of CIKs with gvkeys
inevitably results in the loss of some observations. Ultimately, for the
comparison the clustering algorithm was applied to 1,958 of the 3,246
firms in our sample for the year 2017. In our dataset, companies are
from 320 different SIC codes. Therefore, for the comparison the number
of industries chosen for our industry classification and the FIC is 320.

Figure 2 compares the distribution of industry size for the BERTScore
classification, the FIC and the SIC. Both the BERTScore classification
and the FIC show a similar distribution, displaying a leftward skew with
the majority of industries comprising fewer than ten firms. The SIC
shows as well a left skewed distribution but with most industries only
containing one company. The distribution of the FIC is steeper than the
one of the BERTScore classification. It is notable that the largest
industry in the BERTScore classification comprises only 20 companies,
whereas the FIC and SIC contain industries with a greater number of
firms, with some exceeding 50. This suggests that the BERTScore
classification groups small to medium-sized industries, comprising
between two and fourteen firms per industry, with fewer large
industries. The FIC also comprises mostly of small to medium-sized
industries, with a few larger ones. Despite these minor differences,
this supports H1. The degree of homogeneity between the BERTScore
classification and the FIC is 0.63, while the completeness is 0.6. This
demonstrates only a medium degree of overlap between the two
classifications. The Adjusted Rand Index (ARI) (Hubert and Arabie 1985)
is situated at 0.0002, which is close to zero, indicating that the
overlap might be random. These findings do not provide support for H2.

In order to use our BERTScore industry classification in our estimation,
we classify all 3,246 companies from the year 2017 as described above.
Since we use BERT and do not have word vectors for each industry, our
methodology differs in the second step from Hoberg \& Phillips (2016).
We assign the remaining companies of which we do not have data for the
year 2017 to the industry of the company which are already classified
and which they are most similar to.

\begin{figure}

\centering{

\includegraphics{ProjectEcoDataScience_files/figure-pdf/fig-2-1.pdf}

}

\caption{\label{fig-2}Distribution Comparison between BERT
Classification, FIC and SIC}

\end{figure}%

\newpage{}

\subsection{Results}\label{results}

Table 3 presents the four regressions related to hypothesis H3. As
previously described, this relationship is examined using sales growth
and Tobin's Q growth as firm performance indicators. Models 1 and 2 use
sales growth as the dependent variable. Model 1 shows a significant
positive effect for the Normalized Distance (\(beta\) = 0.289, p
\textless{} 0.01) on the sales growth. The results can be interpreted as
that a company, that changes its BM in one year one percentage point
more than the market average, experiences on average 0.289 percentage
points more growth in sales. Model 2 as well shows this significant
positive effect (\(beta\) = 0.292, p \textless{} 0.01). Both regressions
yield very similar results regardless of whether we include the
logarithm of the market value (\(beta\) = -1.014, p \textgreater{} 0.1)
or the logarithm of the total assets (\(beta\) = -2.155, p \textless{}
0.05) as a control variable for the firm size. These results provide
support for H3.

Model 3 and 4 assess the effect of the Normalized Distance on Tobin's Q
growth. Both Models show a negative significant effect (\(beta\) =
-0.007, p \textless{} 0.001 and \(beta\) = -0.008, p \textless{} 0.001).
This can be interpreted as indicating that a company that changes its BM
by one percentage point more than the market average over the
observation period will experience, on average, a reduction in Tobin's Q
growth of 0.008, respectively 0.007, percentage points. As in Model 1
and 2, the coefficients for the Normalized Distance only change very
little when controlling for the logarithm of the market value instead of
the logarithm of the total assets. The results do not support H3, since
we anticipated a positive effect.

Version 1: Table 4 shows the results of the split model. We find a
significant positive effect (\(beta\) = 0.536, p \textless{} 0.01) for
small-cap companies and no significant effect for mid-cap (\(beta\)
=0.008, p \textgreater{} 0.1) and large-cap (\(beta\) = 0.106, p
\textgreater{} 0.1) companies. These results shows that the positive
effect of BMI is in fact stronger than for mid-cap and large-cap
companies since for both the coefficients are not significant. This
supports H4.

In summary, the findings indicate that BMI has a significant impact on
firm performance. Specifically, BM changes have a positive effect on
operational metrics such as sales growth, which can be viewed as a
positive outcome. A differentiated view of market capitalization shows
that the BMI of smaller companies predominantly has a positive impact on
sales growth. In contrast, medium-sized companies have a negative
effect, while large companies only show a marginal positive effect.
However, the stock market does not respond as favorably to such changes.
Tobin's Q growth decreases when companies adjust their business models,
meaning the ratio of market value to total capital declines. This effect
is not necessarily problematic, as market value is influenced by stock
trading, which incorporates subjective evaluations that are reflected in
this metric.

\begin{table}[H]
\centering\centering
\caption{Regression Results I}
\centering
\resizebox{\ifdim\width>\linewidth\linewidth\else\width\fi}{!}{
\begin{tabular}[t]{lcccc}
\toprule
\multicolumn{1}{c}{ } & \multicolumn{2}{c}{Sales Growth} & \multicolumn{2}{c}{Tobin's Q Growth} \\
\cmidrule(l{3pt}r{3pt}){2-3} \cmidrule(l{3pt}r{3pt}){4-5}
  & (1) & (2) & (3) & (4)\\
\midrule
Normalized Distance & \num{0.289}** & \num{0.292}** & \num{-0.007}*** & \num{-0.008}***\\
 & (\num{0.110}) & (\num{0.110}) & (\num{0.001}) & (\num{0.001})\\
log(Market Value) & \num{-1.014} &  &  & \\
 & (\num{0.921}) &  &  & \\
log(Assets) &  & \num{-2.155}* &  & \\
 &  & (\num{0.927}) &  & \\
log(Mean Market Value) &  &  & \num{0.022}* & \\
 &  &  & (\num{0.011}) & \\
log(Mean Assets) &  &  &  & \num{-0.001}\\
 &  &  &  & (\num{0.010})\\
\midrule
Industry x Year Fixed Effects & Yes & Yes & No & No\\
Industry Fixed Effects & No & No & Yes & Yes\\
Time Difference Fixed Effects & No & No & Yes & Yes\\
Num.Obs. & \num{12155} & \num{12155} & \num{3341} & \num{3341}\\
R2 & \num{0.248} & \num{0.248} & \num{0.196} & \num{0.195}\\
RMSE & \num{201.04} & \num{200.99} & \num{1.26} & \num{1.26}\\
\bottomrule
\multicolumn{5}{l}{\rule{0pt}{1em}\textit{Note: }}\\
\multicolumn{5}{l}{\rule{0pt}{1em}Industry classification by BERTScore.}\\
\multicolumn{5}{l}{\rule{0pt}{1em}+ p $<$ 0.1, * p $<$ 0.05, ** p $<$ 0.01, *** p $<$ 0.001}\\
\end{tabular}}
\end{table}

\begin{table}[H]
\centering\centering
\caption{Regression Results II}
\centering
\resizebox{\ifdim\width>\linewidth\linewidth\else\width\fi}{!}{
\begin{tabular}[t]{lccc}
\toprule
\multicolumn{1}{c}{ } & \multicolumn{3}{c}{Sales Growth} \\
\cmidrule(l{3pt}r{3pt}){2-4}
  & Small-Cap & Mid-Cap & Large-Cap\\
\midrule
Normalized Distance & \num{0.536}** & \num{0.008} & \num{0.106}\\
 & (\num{0.197}) & (\num{0.254}) & (\num{0.122})\\
log(Assets) & \num{-1.041} & \num{-5.590} & \num{-2.169}\\
 & (\num{1.921}) & (\num{6.188}) & (\num{2.291})\\
\midrule
Industry x Year Fixed Effects & Yes & Yes & Yes\\
Num.Obs. & \num{6573} & \num{2923} & \num{2659}\\
R2 & \num{0.320} & \num{0.411} & \num{0.567}\\
RMSE & \num{223.76} & \num{159.68} & \num{81.49}\\
\bottomrule
\multicolumn{4}{l}{\rule{0pt}{1em}\textit{Note: }}\\
\multicolumn{4}{l}{\rule{0pt}{1em}Industry classification by BERTScore.}\\
\multicolumn{4}{l}{\rule{0pt}{1em}+ p $<$ 0.1, * p $<$ 0.05, ** p $<$ 0.01, *** p $<$ 0.001}\\
\end{tabular}}
\end{table}

Version 2: Table 4 illustrates how the previously demonstrated effects
influences individual firm sizes. In the overall analysis, the effects
are significant and support our hypothesis. When we differentiate the
companies into Small-cap (6,573 observations), Mid-cap (2,923
observations), and Large-cap (2,659 observations), the explanatory
coefficients reveal substantial differences. The small firms exhibit by
far the most significant effect on the altered distance, which is also
significant at the 5\% level. In contrast, for large companies, we
observe only approximately 20\% of the effect of the business model
change on sales figures. Furthermore, the coefficient does not even
exceed one standard deviation, indicating a lack of significance. The
situation is even less favorable for mid-sized companies, as evidenced
by both the coefficient and standard deviation. The value of the
distance effect accounts for merely 8\% of that observed in large
companies, while the standard deviation stands at 0.254, more than twice
as high. Therefore, we can conclude that the previously observed effect
across all firms can primarily be attributed to small firms. For medium
to large firms, no discernible impact is evident.

\subsection{Robustness Checks}\label{robustness-checks}

To check the validity and reliability of this study, we conduct a series
of robustness checks, illustrating alternative models. The revenue
growth and the normalized distance are not logarithmized in this
specification. Revenue growth can assume negative values and is a
relative measure. The normalized distance is also a relative measure,
and the distance itself is characterized by a norm distribution
(according to Figure 1). Logarithmization is therefore not required. The
use of market value and total assets as control variables makes it
possible to control for the size of the company, which results on the
one hand from supply and demand on the capital market, and on the other
hand from the actual book value of the company. The correlation between
these two variables is approximately 0.5. In Table 4, we perform a
regression using SIC codes instead of the industry classification based
on BERTScores as fixed effects.

The number of observations decreases due to missing SIC codes for
non-surviving companies. As a result Table 5 rather tend to contain a
survivor ship bias (the number of observations decreases due to missing
SIC codes for non-surviving companies). In addition, the coefficients
for sales growth are no longer significant, the standard errors of the
coefficients increase despite lower coefficient values. The value for
\(R^2\) also decreases for each model. This suggests that BERT-based
industry classifications as fixed effects likely capture
industry-specific endogenous effects, thereby enhancing both the model's
validity and its informative value.

With regard to Table 4, other threshold values could also have been
selected for a sample split. Each institution (index providers, stock
exchanges, financial analysts, etc.) defines its own thresholds,
although these do not usually vary greatly. In our study, we use a flat
threshold for small-cap and large-cap firms for simplicity, although
definitions for very small or very large firms would also require more
classifications for a sample split. Due to the limited number of
observations, we consider it sensible to dispense with further
classifications.

\begin{table}[H]
\centering\centering
\caption{Robustness Check}
\centering
\resizebox{\ifdim\width>\linewidth\linewidth\else\width\fi}{!}{
\begin{tabular}[t]{lcccc}
\toprule
\multicolumn{1}{c}{ } & \multicolumn{2}{c}{Sales Growth} & \multicolumn{2}{c}{Tobin's Q Growth} \\
\cmidrule(l{3pt}r{3pt}){2-3} \cmidrule(l{3pt}r{3pt}){4-5}
  & (1) & (2) & (3) & (4)\\
\midrule
Normalized Distance & \num{0.180} & \num{0.185} & \num{-0.009}*** & \num{-0.009}***\\
 & (\num{0.129}) & (\num{0.129}) & (\num{0.001}) & (\num{0.001})\\
log(Market Value) & \num{-4.112}*** &  &  & \\
 & (\num{1.010}) &  &  & \\
log(Assets) &  & \num{-5.157}*** &  & \\
 &  & (\num{0.965}) &  & \\
log(Mean Assets) &  &  & \num{0.029}** & \\
 &  &  & (\num{0.010}) & \\
log(Mean Market Value) &  &  &  & \num{0.049}***\\
 &  &  &  & (\num{0.011})\\
Time Difference &  &  & \num{0.098}*** & \num{0.096}***\\
 &  &  & (\num{0.018}) & (\num{0.018})\\
\midrule
Industry x Year Fixed Effects & Yes & Yes & No & No\\
Industry Fixed Effects & No & No & Yes & Yes\\
Num.Obs. & \num{11187} & \num{11187} & \num{2977} & \num{2977}\\
R2 & \num{0.139} & \num{0.140} & \num{0.131} & \num{0.134}\\
RMSE & \num{219.86} & \num{219.71} & \num{1.33} & \num{1.33}\\
\bottomrule
\multicolumn{5}{l}{\rule{0pt}{1em}\textit{Note: }}\\
\multicolumn{5}{l}{\rule{0pt}{1em}Industry classification by SIC codes.}\\
\multicolumn{5}{l}{\rule{0pt}{1em}+ p $<$ 0.1, * p $<$ 0.05, ** p $<$ 0.01, *** p $<$ 0.001}\\
\end{tabular}}
\end{table}

\subsection{Discussion}\label{discussion}

\subsubsection{Interpretation}\label{interpretation}

The findings of this study contribute to the expansion of the
considerations put forth by Hoberg \& Phillips (2016) Using BERT enables
a semantic analysis of 10-K filings, allowing for a comprehensive
company comparison. The categorization into industries can be updated
annually, similar to Hoberg \& Phillips (2016) approach, albeit with a
somewhat greater effort involved. However, this additional effort is
justifiable, as the preliminary work with Gemini facilitates a holistic
examination of the 10-Ks. This aspect is a significant advantage of our
approach, which we believe surpasses the Word-to-Vec method employed by
Hoberg. We fully utilize the description of the BM, although certain
limitations may exist. However, the loss associated with the previous
method is substantially greater, as most words do not contribute to the
analysis at all.

To broaden the scope of this study, we also address the financial
aspects of the companies and how innovations impact these aspects. The
previously applied model will be used to measure changes in the BM. This
allows for the testing of new hypotheses and the generation of
additional insights. In this study, the relationship with performance is
measured using revenue growth and Tobin's Q. We find a significant
dependence of innovation on sales and Tobin's Q, indicating that while
revenue increases, the market value to asset ratio declines with
increased changes in the BM. The result of revenue growth is not
surprising; we anticipated this effect based on the premise that
revenues would rise as companies aim to capture new markets and thereby
generate more sales through changes in their business plans. Conversely,
the decline in Tobin's Q is somewhat unusual. A possible explanation for
this could be that investors trading stocks are often risk-averse,
perceiving the changes negatively rather than viewing them as
opportunities (time-varying risk aversion). Consequently, demand may
decrease, and the stock price may also experience negative shifts.
Furthermore, the profits generated by the company may temporarily
decline due to these decisions. While the company's revenue is projected
to increase on average, innovations may incur additional expenses that
diminish profit and, consequently, dividends. (Business model
innovation: it's not just about technology anymore) This could further
reduce demand for the stock. Additionally, the new BM may create
tensions with the existing one, leading to increased workload and other
implications.

In our further analysis, we differentiate the companies by size to
examine whether the effects of revenue growth vary accordingly. This
assumption is confirmed, as significance is measured exclusively among
smaller firms with a market capitalization of less than one billion.
This implies that changes in the BM do not affect the performance of
revenue growth in medium to large firms. A potential reason for this
could be that smaller companies typically possess a less extensive
product portfolio, making changes to a business segment or the addition
of a new product more impactful.

In conclusion, we find that adjustments to the BM positively influence
revenue growth for small firms. However, it is crucial not to
misinterpret the results for larger firms by deeming such adjustments
unprofitable or nonsensical. This study focuses primarily on short-term
effects, making it more challenging to observe the impact on the
products of larger companies. Long-term profitability resulting from
innovation is acknowledged but falls outside the scope of this analysis.

\subsubsection{Limitation}\label{limitation}

This study represents an innovative extension of Hoberg \& Phillips
(2016) work regarding their approach. While using the BERTScore compels
us to shorten and condense the descriptions of the BM, it integrates
semantics into the similarity calculation. As a result, the entire text
is considered, which appears to provide better informational value
compared to solely relying on frequently occurring nouns. However, one
potential critique of BERT is its limited ability to recognize implicit
and subtle meanings (Acheampong, Nunoo-Mensah, and Chen 2021). The
approach can only process the textual descriptions. In this work, we
exclusively examine business models that provide little insight into
current success in practice. Notably, there are no clear objective
statements, as with many key financial indicators, that can be
categorized as positive or negative (George and Bock 2011). A positive
description of negative aspects is unlikely in this context, which
justifies the use of the BERTScore.

The use of BERT, and the consequent limitation regarding text length,
posed additional challenges in this research. In recent years, the
possibilities for utilizing AI have grown immensely. With the help of
GitHub Actions, we were able to obtain suitable access to Gemini. This
brings us to the most critical point of this work. This approach raises
transparency issues for the results. The system does not provide
explainable intelligence, meaning we cannot fully verify how exactly the
texts are generated. Our only option is to delegate the task and trust
the results' accuracy.

Nevertheless, we consider the current approach as the best possible
solution for condensing our BMs. On the one hand, the extracted texts do
not follow the same structure across companies, and on the other,
cutting the business models arbitrarily poses too great a risk. Losing
up to 90 percent of the content would be unacceptable in this case, and
due to structural differences, this becomes impossible beyond certain
key points. Moreover, the summaries generated by GPT have sometimes been
perceived as better than models specifically designed for this purpose
(Goyal, Li, and Durrett 2023). A significant portion of this work
involved data collection, which proved to be a major challenge. Due to
the lack of access to Compustat, commonly used in financial studies and
for SEC-related information, we had to rely on extensive research.
Various approaches had to be abandoned as they failed to meet
expectations. Nevertheless, we were able to complete our work, albeit
with some compromises regarding the amount of data.

In conclusion, the findings of our study suggest that BMI is associated
with improved performance, specifically in terms of operating revenue.
However, we can only report a correlation rather than a causal effect,
as the complexity of organizations is substantial, and our dataset does
not allow for definitive conclusions regarding causality. Nonetheless,
this study can be viewed as a continuation of the approach by Hoberg \&
Phillips (2016), and the examination of BMI's impact on performance
could be further explored in future studies, offering valuable
extensions to this research.

\section{Conclusion}\label{conclusion}

The present study builds on the work of Hoberg \& Phillips (2016) and
extends their approach to improving traditional industry classifications
by analyzing product descriptions in the context of BM. This research
leverages modern methodologies to incorporate entire 10-K filings into
the analysis. Unlike Hoberg \& Phillips (2016), who focus on frequent
nouns in a Word-to-Vec model, we employ BERT, which allows us not only
to analyze the entirety of the 10-Ks but also to consider their semantic
meaning. However, due to BERT's token length restrictions, we also use
the AI tool Gemini to adjust the length of the 10-Ks. To remain
consistent with the methodology of the referenced study, we group the
BERTScores into industries through hierarchical clustering. We
demonstrate that our classification exhibits a distribution similar to
the FIC by Hoberg \& Phillips (2016). Contrary to our expectations,
there is no significant general overlap between the two classification
methods.

Beyond extending the industry classification using 10-K filings, we also
see further potential in the BERTScores. Therefore, we conducted
additional analyses to examine the relationship between BMI and
financial performance. Using regression analysis, we find evidence of a
correlation: greater changes in the BM are associated with higher
revenue growth and a lower Tobin's Q. These results, however, are based
on the overall observations, without distinguishing between companies of
different sizes. Upon further analysis using multiple regressions
segmented by company size, we observe that the effect on revenue growth
is statistically significant only for smaller firms.

In conclusion, we find that smaller firms benefit more from BMI, or in
the short term, are the only ones that show a positive impact.
Additionally, the insights and methodology presented in this study can
be applied to discover new effects of BMI. For instance, an increase in
R\&D spending does not always result in innovation, a distinction that
can be better determined using our new approach.

\section{Acknowledgement}\label{acknowledgement}

\begin{itemize}
\tightlist
\item
  Jonathan for IT Support
\item
  Prof.~Kranz for Repo
\item
  bw unicluster
\end{itemize}

\newpage{}

\section{References}\label{references}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-acheampong_transformer_2021}
Acheampong, Francisca Adoma, Henry Nunoo-Mensah, and Wenyu Chen. 2021.
{``Transformer Models for Text-Based Emotion Detection: A Review of
{BERT}-Based Approaches.''} \emph{Artificial Intelligence Review} 54
(8): 5789--829. \url{https://doi.org/10.1007/s10462-021-09958-2}.

\bibitem[\citeproctext]{ref-clauss_measuring_2017}
Clauss, Thomas. 2017. {``Measuring Business Model Innovation:
Conceptualization, Scale Development, and Proof of Performance.''}
\emph{R\&D Management} 47 (3): 385--403.
\url{https://doi.org/10.1111/radm.12186}.

\bibitem[\citeproctext]{ref-cucculelli_business_2015}
Cucculelli, Marco, and Cristina Bettinelli. 2015. {``Business Models,
Intangibles and Firm Performance: Evidence on Corporate Entrepreneurship
from {Italian} Manufacturing {SMEs}.''} \emph{Small Business Economics}
45 (2): 329--50. \url{https://doi.org/10.1007/s11187-015-9631-7}.

\bibitem[\citeproctext]{ref-dang2018measuring}
Dang, Chongyu, Zhichuan Frank Li, and Chen Yang. 2018. {``Measuring Firm
Size in Empirical Corporate Finance.''} \emph{Journal of Banking \&
Finance} 86: 159--76.

\bibitem[\citeproctext]{ref-Desyllas2022}
Desyllas, Panos, Ammon Salter, and Oliver Alexy. 2022. {``The Breadth of
Business Model Reconfiguration and Firm Performance.''} \emph{Strategic
Organization} 20 (2): 231--69.
\url{https://doi.org/10.1177/1476127020955138}.

\bibitem[\citeproctext]{ref-foss_fifteen_2017}
Foss, Nicolai J., and Tina Saebi. 2017. {``Fifteen {Years} of {Research}
on {Business} {Model} {Innovation}: {How} {Far} {Have} {We} {Come}, and
{Where} {Should} {We} {Go}?''} \emph{Journal of Management} 43 (1):
200--227. \url{https://doi.org/10.1177/0149206316675927}.

\bibitem[\citeproctext]{ref-george_business_2011}
George, Gerard, and Adam J. Bock. 2011. {``The {Business} {Model} in
{Practice} and Its {Implications} for {Entrepreneurship} {Research}.''}
\emph{Entrepreneurship Theory and Practice} 35 (1): 83--111.
\url{https://doi.org/10.1111/j.1540-6520.2010.00424.x}.

\bibitem[\citeproctext]{ref-goyal_news_2023}
Goyal, Tanya, Junyi Jessy Li, and Greg Durrett. 2023. {``News
{Summarization} and {Evaluation} in the {Era} of {GPT}-3.''} arXiv.
\url{http://arxiv.org/abs/2209.12356}.

\bibitem[\citeproctext]{ref-guldmann_barriers_2020}
Guldmann, Eva, and Rikke Dorothea Huulgaard. 2020. {``Barriers to
Circular Business Model Innovation: {A} Multiple-Case Study.''}
\emph{Journal of Cleaner Production} 243 (January): 118160.
\url{https://doi.org/10.1016/j.jclepro.2019.118160}.

\bibitem[\citeproctext]{ref-hoberg_text-based_2016}
Hoberg, Gerard, and Gordon Phillips. 2016. {``Text-{Based} {Network}
{Industries} and {Endogenous} {Product} {Differentiation}.''}
\emph{Journal of Political Economy} 124 (5): 1423--65.
\url{https://doi.org/10.1086/688176}.

\bibitem[\citeproctext]{ref-huang_review_2023}
Huang, WenJun, and Takeyasu Ichikohji. 2023. {``A Review and Analysis of
the Business Model Innovation Literature.''} \emph{Heliyon} 9 (7):
e17895. \url{https://doi.org/10.1016/j.heliyon.2023.e17895}.

\bibitem[\citeproctext]{ref-hubert_comparing_1985}
Hubert, Lawrence, and Phipps Arabie. 1985. {``Comparing Partitions.''}
\emph{Journal of Classification} 2 (1): 193--218.
\url{https://doi.org/10.1007/BF01908075}.

\bibitem[\citeproctext]{ref-latifi_business_2021}
Latifi, Mohammad-Ali, Shahrokh Nikou, and Harry Bouwman. 2021.
{``Business Model Innovation and Firm Performance: {Exploring} Causal
Mechanisms in {SMEs}.''} \emph{Technovation} 107 (September): 102274.
\url{https://doi.org/10.1016/j.technovation.2021.102274}.

\bibitem[\citeproctext]{ref-lee_business_2014}
Lee, Jihwan, and Yoo S. Hong. 2014. {``Business {Model} {Mining}:
{Analyzing} a {Firm}'s {Business} {Model} with {Text} {Mining} of
{Annual} {Report}.''} \emph{Industrial Engineering and Management
Systems} 13 (4): 432--41.
\url{https://doi.org/10.7232/iems.2014.13.4.432}.

\bibitem[\citeproctext]{ref-pucihar_drivers_2019}
Pucihar, Andreja, Gregor Lenart, Mirjana Kljajić Borštnar, Doroteja
Vidmar, and Marjeta Marolt. 2019. {``Drivers and {Outcomes} of
{Business} {Model} {Innovation}---{Micro}, {Small} and {Medium}-{Sized}
{Enterprises} {Perspective}.''} \emph{Sustainability} 11 (2): 344.
\url{https://doi.org/10.3390/su11020344}.

\bibitem[\citeproctext]{ref-sec_investor_2024}
SEC. 2024. {``Investor {Bulletin}: {How} to {Read} a 10-{K}.''}
\url{https://www.sec.gov/files/reada10k.pdf}.

\bibitem[\citeproctext]{ref-spieth_business_2016}
Spieth, Patrick, and Sabrina Schneider. 2016. {``Business Model
Innovativeness: Designing a Formative Measure for Business Model
Innovation.''} \emph{Journal of Business Economics} 86 (6): 671--96.
\url{https://doi.org/10.1007/s11573-015-0794-0}.

\bibitem[\citeproctext]{ref-teece_business_2018}
Teece, David J. 2018. {``Business Models and Dynamic Capabilities.''}
\emph{Long Range Planning} 51 (1): 40--49.
\url{https://doi.org/10.1016/j.lrp.2017.06.007}.

\bibitem[\citeproctext]{ref-vaswani2017attention}
Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion
Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017.
{``Attention Is All You Need.(nips), 2017.''} \emph{arXiv Preprint
arXiv:1706.03762} 10: S0140525X16001837.

\bibitem[\citeproctext]{ref-white_exploring_2022}
White, Joshua V., Erik Markin, David Marshall, and Vishal K. Gupta.
2022. {``Exploring the Boundaries of Business Model Innovation and Firm
Performance: {A} Meta-Analysis.''} \emph{Long Range Planning} 55 (5):
102242. \url{https://doi.org/10.1016/j.lrp.2022.102242}.

\bibitem[\citeproctext]{ref-zott_fit_2008}
Zott, Christoph, and Raphael Amit. 2008. {``The Fit Between Product
Market Strategy and Business Model: Implications for Firm
Performance.''} \emph{Strategic Management Journal} 29 (1): 1--26.
\url{https://doi.org/10.1002/smj.642}.

\end{CSLReferences}

\newpage{}

\section{Appendix}\label{appendix}

\subsection{Appendix A}\label{appendix-a}

We used following prompt: ``Summarize the business model from the
following text. Answer with a continuous text and with five hundred
twelve tokens at max. Set your focus on sources of revenue, the intended
customer base, products, distribution channels and details of financing.
Use only information from the following the text''.\footnote{The
  spelling error in the last sentence of the prompt was found after
  processing Item 1. After evaluating the summaries, this error did not
  cause any issues.} ``Intended customer base'' and ``product'' refer to
the value offering, ``distribution channels'' refers to the value
architecture, and ``sources of revenue'' and ``details of financing''
refer to the revenue model. The term `tokens' was used deliberately in
preference to `words', given that the number of tokens and the number of
words in a text may vary depending on the tokeniser. This way, we wanted
to ensure that the whole summary is used by the BERT model.

Table 6 presents the descriptive measures of the length of our
summaries. On average our summaries are 350 till 371 words long with the
75th percentile ranging between 404 and 432 words. Looking at the
maxima, some summaries are significantly longer than the average with a
couple thousand words. Most of the summaries are short enough to be
processed by BERT as a whole, while some summaries are too long for
BERT. These outliners are only a few and therefore negligible.

\begin{table}[H]
\centering
\caption{Descriptive Statistics of Number of Words of our Summaries}
\centering
\resizebox{\ifdim\width>\linewidth\linewidth\else\width\fi}{!}{
\begin{tabular}[t]{rrrrrrrr}
\toprule
Report-for Year & Average Word Count & Standard Deviation & Minimum & 25th Percentile & Median & 75th Percentile & Maximum\\
\midrule
2016 & 355 & 156 & 116 & 273 & 331 & 409 & 5377\\
2017 & 356 & 154 & 117 & 274 & 331 & 407 & 5951\\
2018 & 350 & 109 & 31 & 275 & 330 & 404 & 1057\\
2019 & 358 & 186 & 56 & 276 & 333 & 407 & 6273\\
2020 & 367 & 214 & 56 & 276 & 337 & 422 & 6872\\
\addlinespace
2021 & 370 & 159 & 76 & 281 & 344 & 432 & 5559\\
2022 & 371 & 161 & 138 & 283 & 345 & 425 & 6361\\
2023 & 366 & 207 & 150 & 275 & 333 & 419 & 5154\\
\bottomrule
\multicolumn{8}{l}{\rule{0pt}{1em}\textit{Note: }}\\
\multicolumn{8}{l}{\rule{0pt}{1em}All 21,417 summaries were considered.}\\
\end{tabular}}
\end{table}

\subsection{Appendix B}\label{appendix-b}

To check the validity of the summaries generated by Gemini, we took a
random sample of ten filings to evaluate them manually. We evaluated the
summaries according to the criteria ``Accuracy'' (how well does the
summary reflect the unabridged content), ``Relevance'' (how relevant is
the information from the summary) and ``Coherence'' (is the summary
logically structured and comprehensible). We use a rating scale from one
to ten. The results are shown in Table 7.

\begin{table}[H]
\centering
\caption{Subjective Evaluation of Random Item 1 Summaries}
\centering
\resizebox{\ifdim\width>\linewidth\linewidth\else\width\fi}{!}{
\begin{tabular}[t]{lrrrr}
\toprule
Company Name & Year & Accuracy & Relevance & Coherence\\
\midrule
COMMUNICATIONS SYSTEMS INC & 2016 & 9.0 & 9.5 & 8.5\\
NanoVibronix, Inc. & 2018 & 9.5 & 8.0 & 8.0\\
GameStop Corp. & 2018 & 9.0 & 8.0 & 9.0\\
FOCUS UNIVERSAL INC. & 2020 & 8.0 & 9.0 & 9.5\\
BIG LOTS INC & 2018 & 9.0 & 8.0 & 8.0\\
\addlinespace
WEX Inc. & 2022 & 9.0 & 8.0 & 9.5\\
COPART INC & 2023 & 8.5 & 8.0 & 8.5\\
Data Storage Corp & 2019 & 8.5 & 9.0 & 9.5\\
AGILYSYS INC & 2017 & NA & NA & NA\\
Kosmos Energy Ltd. & 2020 & NA & NA & NA\\
\bottomrule
\multicolumn{5}{l}{\rule{0pt}{1em}\textit{Note: }}\\
\multicolumn{5}{l}{\rule{0pt}{1em}A random sample of ten filings was drawn}\\
\end{tabular}}
\end{table}

We find that the summaries are valid overall and consistently reflect
the BM. It must also be taken into account that the evaluation criteria
``Accuracy'' and ``Relevance'' are to a certain extent in a trade-off
situation. A suitable optimum was found in all summaries. One
conspicuous feature was that although strong diversification tends to be
mentioned in the summaries, no possible interaction between these areas
is discussed.

\subsection{Appendix C}\label{appendix-c}

In our second dataset we winsorize only the top quantile. Figure 3
presents a boxplot of the distribution of the sales growth and also
explains our reasoning behind this decision. The boxplot shows that the
big majority of observations has a growth rate below 1,000 percent.
Sales growth rates of multiple ten thousand percent or even of a couple
million percent seem very unrealistic and are probably due to poor data
quality. By winsorizing at the top one percent quantile, we get ride of
these unrealistically high values. For the sake of completeness, Table 8
reports the regression results without winsorizing the data. All
coefficients are not significant anymore while their values and their
standard errors increase massively.

\begin{figure}

\centering{

\includegraphics{ProjectEcoDataScience_files/figure-pdf/fig-3-1.pdf}

}

\caption{\label{fig-3}Distribution of the Sales Growth}

\end{figure}%

\begin{table}[H]
\centering\centering
\caption{Regression Results (Not Winsorized)}
\centering
\resizebox{\ifdim\width>\linewidth\linewidth\else\width\fi}{!}{
\begin{tabular}[t]{lcc}
\toprule
\multicolumn{1}{c}{ } & \multicolumn{2}{c}{Sales Growth} \\
\cmidrule(l{3pt}r{3pt}){2-3}
  & (1) & (2)\\
\midrule
Normalized Distance & \num{41.999} & \num{41.800}\\
 & (\num{50.080}) & (\num{50.087})\\
log(Market Value) & \num{581.110} & \\
 & (\num{420.543}) & \\
log(Assets) &  & \num{330.692}\\
 &  & (\num{423.289})\\
\midrule
Num.Obs. & \num{12155} & \num{12155}\\
Industry x Year Fixed Effects & Yes & Yes\\
R2 & \num{0.458} & \num{0.458}\\
RMSE & \num{91778.53} & \num{91784.54}\\
\bottomrule
\multicolumn{3}{l}{\rule{0pt}{1em}\textit{Note: }}\\
\multicolumn{3}{l}{\rule{0pt}{1em}This is the same estimation as in Table 3, but without winsorizing the data.}\\
\multicolumn{3}{l}{\rule{0pt}{1em}+ p $<$ 0.1, * p $<$ 0.05, ** p $<$ 0.01, *** p $<$ 0.001}\\
\end{tabular}}
\end{table}



\end{document}
